{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the using package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import requests\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import monpa\n",
    "import random\n",
    "import pickle\n",
    "import torch.nn.init as weight_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, nn.GRU):\n",
    "        nn.init.orthogonal_(m.all_weights)\n",
    "        nn.init.orthogonal_(m.all_bias)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Architecture of the Designed Model using only Bi-GRU \n",
    "<img src=\"https://i.imgur.com/wOizgXZ.png\" width = \"300\" height = \"200\" alt=\"design_model\" align=center />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bi_GRU_Model (nn.Module):\n",
    "    def __init__(self, total_tag, embeding_size, gru_hidden,gru_layer, output_size):\n",
    "        super(Bi_GRU_Model, self).__init__()\n",
    "        self.total_tag = total_tag\n",
    "        self.embeding_size = embeding_size\n",
    "        self.gru_hidden = gru_hidden\n",
    "        self.gru_layer = gru_layer\n",
    "        self.output_size = output_size\n",
    "        self.Embeding = nn.Embedding(self.total_tag, self.embeding_size)\n",
    "        self.GRU = nn.GRU(self.embeding_size , self.gru_hidden, self.gru_layer, bidirectional= True)\n",
    "        weight_init.orthogonal_(self.GRU.weight_ih_l0)\n",
    "        weight_init.orthogonal_(self.GRU.weight_hh_l0)\n",
    "        # use zero init for GRU layer0 bias\n",
    "        #self.GRU.bias_ih_l0.zero_()\n",
    "        #self.GRU.bias_hh_l0.zero_()\n",
    "        \n",
    "        # batch size is  set to 1 , output_size is set to 2\n",
    "        self.linear = nn.Linear( self.gru_hidden*2 , self.output_size )\n",
    "        \n",
    "    def forward(self, input_data):  \n",
    "        embed_res = self.Embeding (input_data)\n",
    "        #print ('Embedding :::: ' , embed_res.size())\n",
    "        # let the input dimension be the (L , N , M) and the get the output with the dimension (L , N , H)\n",
    "        gru_res, _ = self.GRU(embed_res.unsqueeze(1))\n",
    "        #print (\"GRU :::: \", gru_res.size())\n",
    "        linear_res = self.linear(gru_res[-1])\n",
    "        final_res = F.log_softmax(linear_res)\n",
    "        #print ('final res :::: ' , final_res)\n",
    "        return final_res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V1_Model_with_only_gru ::: \n",
      " Bi_GRU_Model(\n",
      "  (Embeding): Embedding(16, 8)\n",
      "  (GRU): GRU(8, 6, num_layers=4, bidirectional=True)\n",
      "  (linear): Linear(in_features=12, out_features=7, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:27: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "model_v1 = Bi_GRU_Model(total_tag=16, embeding_size=8, gru_hidden= 6 , gru_layer=4, output_size=7 )\n",
    "#model_v1.GRU.apply(weights_init)\n",
    "test_vec = torch.LongTensor([1,2,3,4,5,6,7])\n",
    "model_v1(test_vec)\n",
    "print ('V1_Model_with_only_gru ::: \\n', model_v1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing\n",
    "\n",
    "* 將positive_data和negative_data載入存入List\n",
    "* 將monpa的tags載入dictionary\n",
    "* 將sentence 的 words 轉成詞性\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Read both the postive data and negative data\\nwith open ('../CJX_Train_Test_Data/positive_data.txt', 'r', encoding = 'utf-8') as rf:\\n    for pos_s in rf.readlines():\\n        training_data_sentence_list.append(pos_s.strip('\\n').strip(',').strip('，').strip('《').strip('》').strip('【').strip('】').strip('、').strip('、').strip('-'))\\n        training_target_sentence_list.append(1)\\nwith open ('../CJX_Train_Test_Data/positive_data.txt', 'r', encoding = 'utf-8') as rf:\\n    for pos_s in rf.readlines():\\n        training_data_sentence_list.append(pos_s.strip('\\n').strip(',').strip('，').strip('《').strip('》').strip('【').strip('】').strip('、').strip('-'))\\n        training_target_sentence_list.append(0)\\n        \\nprint ('training data size : ',len(training_data_sentence_list))\\nprint ('training data target size : ',len(training_target_sentence_list))\\ntmp_cnt = 0\\n# Change the words to tags, because we need to use the sequential tags for training.\\nfor sentence in training_data_sentence_list:\\n    tmp_cnt += 1\\n    #print (tmp_cnt)\\n    tmp = monpa.pseg(sentence)\\n    tmp_list = list()\\n    for item in tmp:\\n        tmp_list.append(item[1])\\n    training_data_tag_list.append(tmp_list)\\n    \\nprint (len(training_data_tag_list))\\n\\n# Read the tags of the monpa\\nwith open ('./monpa_tag.txt' , 'r' , encoding = 'utf-8') as rf:\\n    cnt = 0\\n    for tag in rf.readlines():\\n        dict_idx2tag[cnt] = tag.strip('\\n')\\n        dict_tag2idx[tag.strip('\\n')] = cnt\\n        cnt += 1  \\n\\nprint ('The dictionary of idxs 2 tags : ', dict_idx2tag)\\nprint ('The dictionary of tags 2 idxs : ' , dict_tag2idx)\\n\\nshuffle_zip = list(zip(training_data_tag_list, training_target_sentence_list))\\nrandom.shuffle(shuffle_zip)\\ntraining_data_tag_list[:], training_target_sentence_list[:] = zip(*shuffle_zip)\\nprint (training_data_tag_list, training_target_sentence_list)\\n\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "training_data_sentence_list = list()\n",
    "training_data_tag_list = list()\n",
    "training_target_sentence_list = list()\n",
    "dict_idx2tag = dict()\n",
    "dict_tag2idx = dict()\n",
    "\n",
    "with open ('../CJX_Train_Test_Data/try_train_p_v2.txt', 'r', encoding = 'utf-8') as rf:\n",
    "    for pos_s in rf.readlines():\n",
    "        training_data_sentence_list.append(pos_s.strip('\\n').strip(',').strip('，').strip('《').strip('》').strip('【').strip('】').strip('、').strip('、').strip('-'))\n",
    "        training_target_sentence_list.append(1)\n",
    "with open ('../CJX_Train_Test_Data/try_train_n.txt', 'r', encoding = 'utf-8') as rf:\n",
    "    for pos_s in rf.readlines():\n",
    "        training_data_sentence_list.append(pos_s.strip('\\n').strip(',').strip('，').strip('《').strip('》').strip('【').strip('】').strip('、').strip('-'))\n",
    "        training_target_sentence_list.append(0)\n",
    "tmp_cnt = 0\n",
    "# Change the words to tags, because we need to use the sequential tags for training.\n",
    "for sentence in training_data_sentence_list:\n",
    "    tmp_cnt += 1\n",
    "    #print (tmp_cnt)\n",
    "    tmp = monpa.pseg(sentence)\n",
    "    tmp_list = list()\n",
    "    for item in tmp:\n",
    "        tmp_list.append(item[1])\n",
    "    training_data_tag_list.append(tmp_list)\n",
    "    \n",
    "print (len(training_data_tag_list))\n",
    "'''\n",
    "'''\n",
    "# Read both the postive data and negative data\n",
    "with open ('../CJX_Train_Test_Data/positive_data.txt', 'r', encoding = 'utf-8') as rf:\n",
    "    for pos_s in rf.readlines():\n",
    "        training_data_sentence_list.append(pos_s.strip('\\n').strip(',').strip('，').strip('《').strip('》').strip('【').strip('】').strip('、').strip('、').strip('-'))\n",
    "        training_target_sentence_list.append(1)\n",
    "with open ('../CJX_Train_Test_Data/positive_data.txt', 'r', encoding = 'utf-8') as rf:\n",
    "    for pos_s in rf.readlines():\n",
    "        training_data_sentence_list.append(pos_s.strip('\\n').strip(',').strip('，').strip('《').strip('》').strip('【').strip('】').strip('、').strip('-'))\n",
    "        training_target_sentence_list.append(0)\n",
    "        \n",
    "print ('training data size : ',len(training_data_sentence_list))\n",
    "print ('training data target size : ',len(training_target_sentence_list))\n",
    "tmp_cnt = 0\n",
    "# Change the words to tags, because we need to use the sequential tags for training.\n",
    "for sentence in training_data_sentence_list:\n",
    "    tmp_cnt += 1\n",
    "    #print (tmp_cnt)\n",
    "    tmp = monpa.pseg(sentence)\n",
    "    tmp_list = list()\n",
    "    for item in tmp:\n",
    "        tmp_list.append(item[1])\n",
    "    training_data_tag_list.append(tmp_list)\n",
    "    \n",
    "print (len(training_data_tag_list))\n",
    "\n",
    "# Read the tags of the monpa\n",
    "with open ('./monpa_tag.txt' , 'r' , encoding = 'utf-8') as rf:\n",
    "    cnt = 0\n",
    "    for tag in rf.readlines():\n",
    "        dict_idx2tag[cnt] = tag.strip('\\n')\n",
    "        dict_tag2idx[tag.strip('\\n')] = cnt\n",
    "        cnt += 1  \n",
    "\n",
    "print ('The dictionary of idxs 2 tags : ', dict_idx2tag)\n",
    "print ('The dictionary of tags 2 idxs : ' , dict_tag2idx)\n",
    "\n",
    "shuffle_zip = list(zip(training_data_tag_list, training_target_sentence_list))\n",
    "random.shuffle(shuffle_zip)\n",
    "training_data_tag_list[:], training_target_sentence_list[:] = zip(*shuffle_zip)\n",
    "print (training_data_tag_list, training_target_sentence_list)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the training data list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nf = open ('../pickle/training_tags.pkl', 'wb')\\npickle.dump(training_data_tag_list, f)\\nf.close()\\nf = open ('../pickle/target.pkl', 'wb')\\npickle.dump(training_target_sentence_list, f)\\nf.close()\\n\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "f = open ('../pickle/training_tags.pkl', 'wb')\n",
    "pickle.dump(training_data_tag_list, f)\n",
    "f.close()\n",
    "f = open ('../pickle/target.pkl', 'wb')\n",
    "pickle.dump(training_target_sentence_list, f)\n",
    "f.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open ('../pickle/training_tags.pkl', 'rb')\n",
    "a = pickle.load( f)\n",
    "f.close()\n",
    "f = open ('../pickle/target.pkl', 'rb')\n",
    "b = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_idx2tag = dict()\n",
    "dict_tag2idx = dict()\n",
    "with open ('./monpa_tag.txt' , 'r' , encoding = 'utf-8') as rf:\n",
    "    cnt = 0\n",
    "    for tag in rf.readlines():\n",
    "        dict_idx2tag[cnt] = tag.strip('\\n')\n",
    "        dict_tag2idx[tag.strip('\\n')] = cnt\n",
    "        cnt += 1  \n",
    "training_data_tag_list = a\n",
    "training_target_sentence_list = b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparamters_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_v1 = 1000\n",
    "learning_rate_v1 = 0.1\n",
    "input_size_v1 = len(dict_idx2tag.keys())\n",
    "gru_hidden_size_v1 = 12\n",
    "embedding_size_v1 = 25\n",
    "output_size_v1 = 2\n",
    "gru_layer_v1 = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定義訓練function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_v1():\n",
    "    \n",
    "    model_v1 = Bi_GRU_Model(total_tag=input_size_v1, \n",
    "                            embeding_size = embedding_size_v1,\n",
    "                            gru_hidden=gru_hidden_size_v1,\n",
    "                            output_size=output_size_v1,\n",
    "                            gru_layer = gru_layer_v1)\n",
    "    \n",
    "    #model_v1 = torch.load('../pickle/model_v1.pt')\n",
    "    #model_v1.apply(weights_init)\n",
    "    optimizer_v1 = torch.optim.Adam(model_v1.parameters(), lr=learning_rate_v1)\n",
    "    loss_function_v1 = nn.NLLLoss()\n",
    "    \n",
    "    for epoch in range(epoch_v1):\n",
    "        sub_epoch = 0\n",
    "        total_loss = 0\n",
    "        #print (len(training_data_tag_list))\n",
    "        for seq_tag_idx in range(len(training_data_tag_list)):\n",
    "            sub_epoch += 1\n",
    "            input_data = torch.LongTensor([dict_tag2idx[t] for t in training_data_tag_list[seq_tag_idx]])\n",
    "            target_data = torch.LongTensor([training_target_sentence_list[seq_tag_idx]])\n",
    "            out = model_v1(input_data)\n",
    "            loss = loss_function_v1(out, target_data)\n",
    "            total_loss += loss.item()\n",
    "            optimizer_v1.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer_v1.step()\n",
    "            '''\n",
    "            if sub_epoch % 100 == 0:\n",
    "                print ('Epoch '+  str(epoch) + ' '+ str(sub_epoch)+'/' + str(len(training_data_tag_list)) + '  Loss : ' + str(loss))\n",
    "            '''\n",
    "        print ('Epoch ' + str(epoch) + ' Loss : ' + str (total_loss))\n",
    "        torch.save(model_v1, '../pickle/model_v1.pt')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:27: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss : 6253.443639934063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type Bi_GRU_Model. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss : 5866.2884086072445\n",
      "Epoch 2 Loss : 6372.107477039099\n",
      "Epoch 3 Loss : 6085.57227486372\n",
      "Epoch 4 Loss : 6052.600871503353\n",
      "Epoch 5 Loss : 6018.86958450079\n",
      "Epoch 6 Loss : 6053.029493749142\n",
      "Epoch 7 Loss : 6053.520365715027\n",
      "Epoch 8 Loss : 6077.569500207901\n",
      "Epoch 9 Loss : 6023.850392192602\n",
      "Epoch 10 Loss : 6048.98771110177\n",
      "Epoch 11 Loss : 5899.0039520561695\n",
      "Epoch 12 Loss : 5981.563179045916\n",
      "Epoch 13 Loss : 6027.671491086483\n",
      "Epoch 14 Loss : 5796.124559104443\n",
      "Epoch 15 Loss : 5719.830504864454\n",
      "Epoch 16 Loss : 6161.539687782526\n",
      "Epoch 17 Loss : 5728.831494361162\n",
      "Epoch 18 Loss : 5698.584744751453\n",
      "Epoch 19 Loss : 5558.6439997553825\n",
      "Epoch 20 Loss : 5517.332997828722\n",
      "Epoch 21 Loss : 5490.901919007301\n",
      "Epoch 22 Loss : 5505.334994316101\n",
      "Epoch 23 Loss : 5505.096551537514\n",
      "Epoch 24 Loss : 5499.073845028877\n"
     ]
    }
   ],
   "source": [
    "train_v1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict():\n",
    "    model_v1 = Bi_GRU_Model(total_tag=input_size_v1, \n",
    "                            embeding_size = embedding_size_v1,\n",
    "                            gru_hidden=gru_hidden_size_v1,\n",
    "                            output_size=output_size_v1,\n",
    "                            gru_layer = gru_layer_v1)\n",
    "    model_v1 = torch.load('../pickle/model_v1.pt')\n",
    "    \n",
    "\n",
    "    correct = 0\n",
    "    for seq_tag_idx in range(len(training_data_tag_list)):\n",
    "        \n",
    "        input_data = torch.LongTensor([dict_tag2idx[t] for t in training_data_tag_list[seq_tag_idx]])\n",
    "        target_data = torch.LongTensor([training_target_sentence_list[seq_tag_idx]])\n",
    "        out = model_v1(input_data)\n",
    "        out = out.squeeze(0)\n",
    "        if out[0].item() > out[1].item() and target_data[0].item() == 0:\n",
    "            correct += 1\n",
    "        elif out[0].item() < out[1].item() and target_data[0].item() == 1:\n",
    "            correct += 1\n",
    "        else : \n",
    "            continue\n",
    "    print (correct / len (training_data_tag_list))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:27: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
