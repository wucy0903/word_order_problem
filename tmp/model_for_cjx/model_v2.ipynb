{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the using package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/25/2019 21:11:54 - INFO - monpa -   running on device cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------+\n",
      "  Welcome to MONPA: Multi-Objective NER POS Annotator for Chinese\n",
      "+---------------------------------------------------------------------+\n",
      "Good, we can find the model file.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import requests\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import monpa\n",
    "import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Architecture of the Designed Model using only Bi-GRU \n",
    "<img src=\"https://i.imgur.com/zjsZb0l.png\" width = \"300\" height = \"200\" alt=\"design_model\" align=center />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolution_1D_Model (nn.Module):\n",
    "    def __init__(self, total_tag, embeding_size, filter_num_1, filter_size_1, filter_num_2, filter_size_2 , output_size,max_word):\n",
    "        super(Convolution_1D_Model, self).__init__()\n",
    "        self.total_tag = total_tag\n",
    "        self.max_word = max_word\n",
    "        self.embeding_size = embeding_size\n",
    "        self.filter_num_1 = filter_num_1\n",
    "        self.filter_size_1 = filter_size_1\n",
    "        self.filter_num_2 = filter_num_2\n",
    "        self.filter_size_2 = filter_size_2\n",
    "        self.output_size = output_size\n",
    "        self.Embeding = nn.Embedding(self.total_tag, self.embeding_size)\n",
    "        self.Conv1d_layer1 = nn.Sequential(nn.Conv1d(in_channels=self.embeding_size, \n",
    "                                                            out_channels=self.filter_num_1,\n",
    "                                                            kernel_size=self.filter_size_1),\n",
    "                                                         #nn.BatchNorm1d (self.filter_size_1),\n",
    "                                                         nn.ReLU())\n",
    "        \n",
    "        self.Conv1d_layer2 = nn.Sequential(nn.Conv1d(in_channels=self.filter_num_1, \n",
    "                                                            out_channels=self.filter_num_2,\n",
    "                                                            kernel_size=self.filter_size_2),\n",
    "                                                         #nn.BatchNorm1d (self.filter_size_2),\n",
    "                                                         nn.ReLU())\n",
    "        self.Conv1d_layer3 = nn.Sequential(nn.Conv1d(in_channels=self.filter_num_2, \n",
    "                                                            out_channels=self.filter_num_2,\n",
    "                                                            kernel_size=self.filter_size_2),\n",
    "                                                         #nn.BatchNorm1d (self.filter_size_2),\n",
    "                                                         nn.ReLU())\n",
    "        self.Conv1d_layer4 = nn.Sequential(nn.Conv1d(in_channels=self.filter_num_2, \n",
    "                                                            out_channels=self.filter_num_2,\n",
    "                                                            kernel_size=self.filter_size_2),\n",
    "                                                         #nn.BatchNorm1d (self.filter_size_2)\n",
    "                                                         nn.ReLU())\n",
    "        self.Conv1d_layer5 = nn.Sequential(nn.Conv1d(in_channels=self.filter_num_2, \n",
    "                                                            out_channels=self.filter_num_2,\n",
    "                                                            kernel_size=self.filter_size_2),\n",
    "                                                         #nn.BatchNorm1d (self.filter_size_2),\n",
    "                                                         nn.ReLU())\n",
    "        self.Conv1d_layer6 = nn.Sequential(nn.Conv1d(in_channels=self.filter_num_2, \n",
    "                                                            out_channels=self.filter_num_2,\n",
    "                                                            kernel_size=self.filter_size_2),\n",
    "                                                         #nn.BatchNorm1d (self.filter_size_2),\n",
    "                                                         nn.ReLU())\n",
    "        \n",
    "        # batch size is  set to 1 , output_size is set to 2\n",
    "        L = self.max_word- self.filter_size_1+ 1\n",
    "        for _ in range (4):\n",
    "            L = L - self.filter_size_2 +1\n",
    "        self.linear = nn.Linear( self.filter_num_2 * L, self.output_size )\n",
    "        \n",
    "    def forward(self, input_data):  \n",
    "        embed_res = self.Embeding (input_data)\n",
    "        if len(input_data) < self.max_word:\n",
    "            for c in range (self.max_word - len(input_data)):\n",
    "                embed_res = torch.cat((embed_res, torch.zeros(1, self.embeding_size)),dim = 0 )\n",
    "        embed_res = embed_res.unsqueeze(0)\n",
    "        #print (' Embedding :::: ' , embed_res.size())\n",
    "        # let the input dimension be the (N , Cin , L) and then the output with the dimension (N, Cout , Lout)\n",
    "        conv_res1 = self.Conv1d_layer1 (embed_res.permute(0,2,1))\n",
    "        #print (' conv_res1 :::: ' , conv_res1.size())\n",
    "        conv_res2 = self.Conv1d_layer2 (conv_res1)\n",
    "        conv_res3 = self.Conv1d_layer3 (conv_res2)\n",
    "        conv_res4 = self.Conv1d_layer4 (conv_res3)\n",
    "        conv_res5 = self.Conv1d_layer5 (conv_res4)\n",
    "        #print ('conv_res2 :::: ' , conv_res2.size())\n",
    "        linear_res = self.linear(conv_res5.view(-1))\n",
    "        final_res = F.log_softmax(linear_res)\n",
    "        #print (\"final_res :::: \" , final_res)\n",
    "        return final_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V2_Model_with_only_convolution1D ::: \n",
      " Convolution_1D_Model(\n",
      "  (Embeding): Embedding(16, 8)\n",
      "  (Conv1d_layer1): Sequential(\n",
      "    (0): Conv1d(8, 5, kernel_size=(2,), stride=(1,))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (Conv1d_layer2): Sequential(\n",
      "    (0): Conv1d(5, 5, kernel_size=(2,), stride=(1,))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (Conv1d_layer3): Sequential(\n",
      "    (0): Conv1d(5, 5, kernel_size=(2,), stride=(1,))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (Conv1d_layer4): Sequential(\n",
      "    (0): Conv1d(5, 5, kernel_size=(2,), stride=(1,))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (Conv1d_layer5): Sequential(\n",
      "    (0): Conv1d(5, 5, kernel_size=(2,), stride=(1,))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (Conv1d_layer6): Sequential(\n",
      "    (0): Conv1d(5, 5, kernel_size=(2,), stride=(1,))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (linear): Linear(in_features=25, out_features=7, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:67: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "model_v2 = Convolution_1D_Model(total_tag=16, embeding_size=8, filter_num_1=5, filter_size_1=2, filter_num_2=5, filter_size_2=2 ,output_size = 7, max_word=10)\n",
    "test_vec = torch.LongTensor([1,2,3,4,5,6,7])\n",
    "model_v2(test_vec)\n",
    "print ('V2_Model_with_only_convolution1D ::: \\n', model_v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing\n",
    "\n",
    "* 將positive_data和negative_data載入存入List\n",
    "* 將monpa的tags載入dictionary\n",
    "* 將sentence 的 words 轉成詞性\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"training_data_sentence_list = list()\\ntraining_data_tag_list = list()\\ntraining_target_sentence_list = list()\\ndict_idx2tag = dict()\\ndict_tag2idx = dict()\\n\\n# Read both the postive data and negative data\\nwith open ('../CJX_Train_Test_Data/positive_data.txt', 'r', encoding = 'utf-8') as rf:\\n    for pos_s in rf.readlines():\\n        training_data_sentence_list.append(pos_s.strip('\\n').strip(',').strip('，').strip('《').strip('》').strip('【').strip('】').strip('、').strip('、').strip('-'))\\n        training_target_sentence_list.append(1)\\nwith open ('../CJX_Train_Test_Data/positive_data.txt', 'r', encoding = 'utf-8') as rf:\\n    for pos_s in rf.readlines():\\n        training_data_sentence_list.append(pos_s.strip('\\n').strip(',').strip('，').strip('《').strip('》').strip('【').strip('】').strip('、').strip('-'))\\n        training_target_sentence_list.append(0)\\n        \\nprint ('training data size : ',len(training_data_sentence_list))\\nprint ('training data target size : ',len(training_target_sentence_list))\\ntmp_cnt = 0\\n# Change the words to tags, because we need to use the sequential tags for training.\\nfor sentence in training_data_sentence_list:\\n    tmp_cnt += 1\\n    #print (tmp_cnt)\\n    tmp = monpa.pseg(sentence)\\n    tmp_list = list()\\n    for item in tmp:\\n        tmp_list.append(item[1])\\n    training_data_tag_list.append(tmp_list)\\n    \\nprint (len(training_data_tag_list))\\n\\n# Read the tags of the monpa\\nwith open ('./monpa_tag.txt' , 'r' , encoding = 'utf-8') as rf:\\n    cnt = 0\\n    for tag in rf.readlines():\\n        dict_idx2tag[cnt] = tag.strip('\\n')\\n        dict_tag2idx[tag.strip('\\n')] = cnt\\n        cnt += 1  \\n\\nprint ('The dictionary of idxs 2 tags : ', dict_idx2tag)\\nprint ('The dictionary of tags 2 idxs : ' , dict_tag2idx)\\n\\nshuffle_zip = list(zip(training_data_tag_list, training_target_sentence_list))\\nrandom.shuffle(shuffle_zip)\\ntraining_data_tag_list[:], training_target_sentence_list[:] = zip(*shuffle_zip)\\nprint (training_data_tag_list, training_target_sentence_list)\\n\""
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''training_data_sentence_list = list()\n",
    "training_data_tag_list = list()\n",
    "training_target_sentence_list = list()\n",
    "dict_idx2tag = dict()\n",
    "dict_tag2idx = dict()\n",
    "\n",
    "# Read both the postive data and negative data\n",
    "with open ('../CJX_Train_Test_Data/positive_data.txt', 'r', encoding = 'utf-8') as rf:\n",
    "    for pos_s in rf.readlines():\n",
    "        training_data_sentence_list.append(pos_s.strip('\\n').strip(',').strip('，').strip('《').strip('》').strip('【').strip('】').strip('、').strip('、').strip('-'))\n",
    "        training_target_sentence_list.append(1)\n",
    "with open ('../CJX_Train_Test_Data/positive_data.txt', 'r', encoding = 'utf-8') as rf:\n",
    "    for pos_s in rf.readlines():\n",
    "        training_data_sentence_list.append(pos_s.strip('\\n').strip(',').strip('，').strip('《').strip('》').strip('【').strip('】').strip('、').strip('-'))\n",
    "        training_target_sentence_list.append(0)\n",
    "        \n",
    "print ('training data size : ',len(training_data_sentence_list))\n",
    "print ('training data target size : ',len(training_target_sentence_list))\n",
    "tmp_cnt = 0\n",
    "# Change the words to tags, because we need to use the sequential tags for training.\n",
    "for sentence in training_data_sentence_list:\n",
    "    tmp_cnt += 1\n",
    "    #print (tmp_cnt)\n",
    "    tmp = monpa.pseg(sentence)\n",
    "    tmp_list = list()\n",
    "    for item in tmp:\n",
    "        tmp_list.append(item[1])\n",
    "    training_data_tag_list.append(tmp_list)\n",
    "    \n",
    "print (len(training_data_tag_list))\n",
    "\n",
    "# Read the tags of the monpa\n",
    "with open ('./monpa_tag.txt' , 'r' , encoding = 'utf-8') as rf:\n",
    "    cnt = 0\n",
    "    for tag in rf.readlines():\n",
    "        dict_idx2tag[cnt] = tag.strip('\\n')\n",
    "        dict_tag2idx[tag.strip('\\n')] = cnt\n",
    "        cnt += 1  \n",
    "\n",
    "print ('The dictionary of idxs 2 tags : ', dict_idx2tag)\n",
    "print ('The dictionary of tags 2 idxs : ' , dict_tag2idx)\n",
    "\n",
    "shuffle_zip = list(zip(training_data_tag_list, training_target_sentence_list))\n",
    "random.shuffle(shuffle_zip)\n",
    "training_data_tag_list[:], training_target_sentence_list[:] = zip(*shuffle_zip)\n",
    "print (training_data_tag_list, training_target_sentence_list)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the training data list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"f = open ('../pickle/training_tags.pkl', 'wb')\\npickle.dump(training_data_tag_list, f)\\nf.close()\\nf = open ('../pickle/target.pkl', 'wb')\\npickle.dump(training_target_sentence_list, f)\\nf.close()\""
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''f = open ('../pickle/training_tags.pkl', 'wb')\n",
    "pickle.dump(training_data_tag_list, f)\n",
    "f.close()\n",
    "f = open ('../pickle/target.pkl', 'wb')\n",
    "pickle.dump(training_target_sentence_list, f)\n",
    "f.close()'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open ('../pickle/training_tags.pkl', 'rb')\n",
    "a = pickle.load(f)\n",
    "f.close()\n",
    "f = open ('../pickle/target.pkl', 'rb')\n",
    "b = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_idx2tag = dict()\n",
    "dict_tag2idx = dict()\n",
    "with open ('./monpa_tag.txt' , 'r' , encoding = 'utf-8') as rf:\n",
    "    cnt = 0\n",
    "    for tag in rf.readlines():\n",
    "        dict_idx2tag[cnt] = tag.strip('\\n')\n",
    "        dict_tag2idx[tag.strip('\\n')] = cnt\n",
    "        cnt += 1  \n",
    "training_data_tag_list = a\n",
    "training_target_sentence_list = b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparamters_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_v2 = 1000\n",
    "learning_rate_v2 = 0.00001\n",
    "input_size_v2 = len(dict_idx2tag.keys())\n",
    "gru_hidden_size_v2 = 8\n",
    "embedding_size_v2 = 8\n",
    "output_size_v2 = 2\n",
    "gru_layer_v2 = 1\n",
    "filter_num_1_v2 = 3\n",
    "filter_size_1_v2 = 2\n",
    "filter_num_2_v2 = 3\n",
    "filter_size_2_v2 = 2\n",
    "max_word_v2 = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定義訓練function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_v2():\n",
    "    model_v2 = Convolution_1D_Model(total_tag= input_size_v2, \n",
    "                                    embeding_size= embedding_size_v2, \n",
    "                                    filter_num_1= filter_num_1_v2, \n",
    "                                    filter_size_1= filter_size_1_v2, \n",
    "                                    filter_num_2= filter_num_2_v2, \n",
    "                                    filter_size_2=filter_size_2_v2 ,\n",
    "                                    output_size = output_size_v2, \n",
    "                                    max_word= max_word_v2 )\n",
    "    optimizer_v2 = torch.optim.SGD(model_v2.parameters(), lr=learning_rate_v2)\n",
    "    loss_function_v2 = nn.NLLLoss()\n",
    "    \n",
    "    for epoch in range(epoch_v2):\n",
    "        sub_epoch = 0\n",
    "        total_loss = 0\n",
    "        for seq_tag_idx in range(len(training_data_tag_list)):\n",
    "            if len(training_data_tag_list[seq_tag_idx]) > max_word_v2:\n",
    "                continue\n",
    "            sub_epoch += 1\n",
    "            input_data = torch.LongTensor([dict_tag2idx[t] for t in training_data_tag_list[seq_tag_idx]])\n",
    "            target_data = torch.LongTensor([training_target_sentence_list[seq_tag_idx]])\n",
    "            out = model_v2(input_data)\n",
    "            out = out.unsqueeze(0)\n",
    "            loss = loss_function_v2(out, target_data)\n",
    "            total_loss += loss.item()\n",
    "            optimizer_v2.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer_v2.step()\n",
    "            '''\n",
    "            if sub_epoch % 100 == 0:\n",
    "                print ('Epoch '+  str(epoch) + ' '+ str(sub_epoch)+'/' + str(len(training_data_tag_list)) + '  Loss : ' + str(total_loss))\n",
    "            '''\n",
    "        print ('Epoch ' + str(epoch) + ' Loss : ' + str (total_loss) + ' total_train_data : ' + str(sub_epoch) )\n",
    "        torch.save(model_v2, '../pickle/model_v2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:67: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss : 4861.99290817976 total_train_data : 7014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type Convolution_1D_Model. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss : 4861.977296233177 total_train_data : 7014\n",
      "Epoch 2 Loss : 4861.962986171246 total_train_data : 7014\n",
      "Epoch 3 Loss : 4861.949198305607 total_train_data : 7014\n",
      "Epoch 4 Loss : 4861.935727536678 total_train_data : 7014\n",
      "Epoch 5 Loss : 4861.922657608986 total_train_data : 7014\n",
      "Epoch 6 Loss : 4861.910870611668 total_train_data : 7014\n",
      "Epoch 7 Loss : 4861.90072286129 total_train_data : 7014\n",
      "Epoch 8 Loss : 4861.891461312771 total_train_data : 7014\n",
      "Epoch 9 Loss : 4861.882511198521 total_train_data : 7014\n",
      "Epoch 10 Loss : 4861.8738723397255 total_train_data : 7014\n",
      "Epoch 11 Loss : 4861.86531996727 total_train_data : 7014\n",
      "Epoch 12 Loss : 4861.85710310936 total_train_data : 7014\n",
      "Epoch 13 Loss : 4861.849670886993 total_train_data : 7014\n",
      "Epoch 14 Loss : 4861.843432128429 total_train_data : 7014\n",
      "Epoch 15 Loss : 4861.837845504284 total_train_data : 7014\n",
      "Epoch 16 Loss : 4861.832608640194 total_train_data : 7014\n",
      "Epoch 17 Loss : 4861.8275300860405 total_train_data : 7014\n",
      "Epoch 18 Loss : 4861.822619378567 total_train_data : 7014\n",
      "Epoch 19 Loss : 4861.81786853075 total_train_data : 7014\n",
      "Epoch 20 Loss : 4861.813086152077 total_train_data : 7014\n",
      "Epoch 21 Loss : 4861.808533251286 total_train_data : 7014\n",
      "Epoch 22 Loss : 4861.804368138313 total_train_data : 7014\n",
      "Epoch 23 Loss : 4861.800847411156 total_train_data : 7014\n",
      "Epoch 24 Loss : 4861.797764837742 total_train_data : 7014\n",
      "Epoch 25 Loss : 4861.7950439453125 total_train_data : 7014\n",
      "Epoch 26 Loss : 4861.792605042458 total_train_data : 7014\n",
      "Epoch 27 Loss : 4861.790214300156 total_train_data : 7014\n",
      "Epoch 28 Loss : 4861.787900030613 total_train_data : 7014\n",
      "Epoch 29 Loss : 4861.7856657505035 total_train_data : 7014\n",
      "Epoch 30 Loss : 4861.783511638641 total_train_data : 7014\n",
      "Epoch 31 Loss : 4861.781424343586 total_train_data : 7014\n",
      "Epoch 32 Loss : 4861.779331386089 total_train_data : 7014\n",
      "Epoch 33 Loss : 4861.777274787426 total_train_data : 7014\n",
      "Epoch 34 Loss : 4861.775288939476 total_train_data : 7014\n",
      "Epoch 35 Loss : 4861.773477435112 total_train_data : 7014\n",
      "Epoch 36 Loss : 4861.771772384644 total_train_data : 7014\n",
      "Epoch 37 Loss : 4861.770323634148 total_train_data : 7014\n",
      "Epoch 38 Loss : 4861.769218623638 total_train_data : 7014\n",
      "Epoch 39 Loss : 4861.768159985542 total_train_data : 7014\n",
      "Epoch 40 Loss : 4861.767185032368 total_train_data : 7014\n",
      "Epoch 41 Loss : 4861.766378939152 total_train_data : 7014\n",
      "Epoch 42 Loss : 4861.765670657158 total_train_data : 7014\n",
      "Epoch 43 Loss : 4861.764997899532 total_train_data : 7014\n",
      "Epoch 44 Loss : 4861.764328122139 total_train_data : 7014\n",
      "Epoch 45 Loss : 4861.7636850476265 total_train_data : 7014\n",
      "Epoch 46 Loss : 4861.763052344322 total_train_data : 7014\n",
      "Epoch 47 Loss : 4861.7624453902245 total_train_data : 7014\n",
      "Epoch 48 Loss : 4861.761854290962 total_train_data : 7014\n",
      "Epoch 49 Loss : 4861.761286854744 total_train_data : 7014\n",
      "Epoch 50 Loss : 4861.760722339153 total_train_data : 7014\n",
      "Epoch 51 Loss : 4861.760200679302 total_train_data : 7014\n",
      "Epoch 52 Loss : 4861.759690403938 total_train_data : 7014\n",
      "Epoch 53 Loss : 4861.7591851353645 total_train_data : 7014\n",
      "Epoch 54 Loss : 4861.758717179298 total_train_data : 7014\n",
      "Epoch 55 Loss : 4861.7582376003265 total_train_data : 7014\n",
      "Epoch 56 Loss : 4861.757715344429 total_train_data : 7014\n",
      "Epoch 57 Loss : 4861.75723773241 total_train_data : 7014\n",
      "Epoch 58 Loss : 4861.756760656834 total_train_data : 7014\n",
      "Epoch 59 Loss : 4861.756299853325 total_train_data : 7014\n",
      "Epoch 60 Loss : 4861.755903601646 total_train_data : 7014\n",
      "Epoch 61 Loss : 4861.755524337292 total_train_data : 7014\n",
      "Epoch 62 Loss : 4861.7551656365395 total_train_data : 7014\n",
      "Epoch 63 Loss : 4861.754851996899 total_train_data : 7014\n",
      "Epoch 64 Loss : 4861.754510939121 total_train_data : 7014\n",
      "Epoch 65 Loss : 4861.75424861908 total_train_data : 7014\n",
      "Epoch 66 Loss : 4861.754038333893 total_train_data : 7014\n",
      "Epoch 67 Loss : 4861.753883898258 total_train_data : 7014\n",
      "Epoch 68 Loss : 4861.753733754158 total_train_data : 7014\n",
      "Epoch 69 Loss : 4861.753624141216 total_train_data : 7014\n",
      "Epoch 70 Loss : 4861.753549516201 total_train_data : 7014\n",
      "Epoch 71 Loss : 4861.753457665443 total_train_data : 7014\n",
      "Epoch 72 Loss : 4861.753370821476 total_train_data : 7014\n",
      "Epoch 73 Loss : 4861.753275334835 total_train_data : 7014\n",
      "Epoch 74 Loss : 4861.753196775913 total_train_data : 7014\n",
      "Epoch 75 Loss : 4861.75313025713 total_train_data : 7014\n",
      "Epoch 76 Loss : 4861.753053247929 total_train_data : 7014\n",
      "Epoch 77 Loss : 4861.753005683422 total_train_data : 7014\n",
      "Epoch 78 Loss : 4861.752951025963 total_train_data : 7014\n",
      "Epoch 79 Loss : 4861.752898275852 total_train_data : 7014\n",
      "Epoch 80 Loss : 4861.752848446369 total_train_data : 7014\n",
      "Epoch 81 Loss : 4861.75281059742 total_train_data : 7014\n",
      "Epoch 82 Loss : 4861.752775490284 total_train_data : 7014\n",
      "Epoch 83 Loss : 4861.752743244171 total_train_data : 7014\n",
      "Epoch 84 Loss : 4861.75272744894 total_train_data : 7014\n",
      "Epoch 85 Loss : 4861.752710342407 total_train_data : 7014\n",
      "Epoch 86 Loss : 4861.752706825733 total_train_data : 7014\n",
      "Epoch 87 Loss : 4861.752671003342 total_train_data : 7014\n",
      "Epoch 88 Loss : 4861.752677500248 total_train_data : 7014\n",
      "Epoch 89 Loss : 4861.752649784088 total_train_data : 7014\n",
      "Epoch 90 Loss : 4861.75264185667 total_train_data : 7014\n",
      "Epoch 91 Loss : 4861.752634465694 total_train_data : 7014\n",
      "Epoch 92 Loss : 4861.752626001835 total_train_data : 7014\n",
      "Epoch 93 Loss : 4861.752620995045 total_train_data : 7014\n",
      "Epoch 94 Loss : 4861.752597093582 total_train_data : 7014\n",
      "Epoch 95 Loss : 4861.752569317818 total_train_data : 7014\n",
      "Epoch 96 Loss : 4861.7525725364685 total_train_data : 7014\n",
      "Epoch 97 Loss : 4861.752592682838 total_train_data : 7014\n",
      "Epoch 98 Loss : 4861.75256228447 total_train_data : 7014\n",
      "Epoch 99 Loss : 4861.7525715231895 total_train_data : 7014\n",
      "Epoch 100 Loss : 4861.752572059631 total_train_data : 7014\n",
      "Epoch 101 Loss : 4861.752546846867 total_train_data : 7014\n",
      "Epoch 102 Loss : 4861.752559185028 total_train_data : 7014\n",
      "Epoch 103 Loss : 4861.75253534317 total_train_data : 7014\n",
      "Epoch 104 Loss : 4861.752539932728 total_train_data : 7014\n",
      "Epoch 105 Loss : 4861.752526640892 total_train_data : 7014\n",
      "Epoch 106 Loss : 4861.752520561218 total_train_data : 7014\n",
      "Epoch 107 Loss : 4861.7525179982185 total_train_data : 7014\n",
      "Epoch 108 Loss : 4861.752516508102 total_train_data : 7014\n",
      "Epoch 109 Loss : 4861.752522110939 total_train_data : 7014\n",
      "Epoch 110 Loss : 4861.752507030964 total_train_data : 7014\n",
      "Epoch 111 Loss : 4861.75250685215 total_train_data : 7014\n",
      "Epoch 112 Loss : 4861.752517819405 total_train_data : 7014\n",
      "Epoch 113 Loss : 4861.752485990524 total_train_data : 7014\n",
      "Epoch 114 Loss : 4861.752501010895 total_train_data : 7014\n",
      "Epoch 115 Loss : 4861.752485513687 total_train_data : 7014\n",
      "Epoch 116 Loss : 4861.752499759197 total_train_data : 7014\n",
      "Epoch 117 Loss : 4861.752475619316 total_train_data : 7014\n",
      "Epoch 118 Loss : 4861.752480447292 total_train_data : 7014\n",
      "Epoch 119 Loss : 4861.752462387085 total_train_data : 7014\n",
      "Epoch 120 Loss : 4861.752479851246 total_train_data : 7014\n",
      "Epoch 121 Loss : 4861.752471148968 total_train_data : 7014\n",
      "Epoch 122 Loss : 4861.752456188202 total_train_data : 7014\n",
      "Epoch 123 Loss : 4861.7524601221085 total_train_data : 7014\n",
      "Epoch 124 Loss : 4861.752460300922 total_train_data : 7014\n",
      "Epoch 125 Loss : 4861.752439081669 total_train_data : 7014\n",
      "Epoch 126 Loss : 4861.752457857132 total_train_data : 7014\n",
      "Epoch 127 Loss : 4861.752426683903 total_train_data : 7014\n",
      "Epoch 128 Loss : 4861.752446651459 total_train_data : 7014\n",
      "Epoch 129 Loss : 4861.752461731434 total_train_data : 7014\n",
      "Epoch 130 Loss : 4861.75244307518 total_train_data : 7014\n",
      "Epoch 131 Loss : 4861.7524290680885 total_train_data : 7014\n",
      "Epoch 132 Loss : 4861.752446115017 total_train_data : 7014\n",
      "Epoch 133 Loss : 4861.752428293228 total_train_data : 7014\n",
      "Epoch 134 Loss : 4861.75245064497 total_train_data : 7014\n",
      "Epoch 135 Loss : 4861.752428412437 total_train_data : 7014\n",
      "Epoch 136 Loss : 4861.75244808197 total_train_data : 7014\n",
      "Epoch 137 Loss : 4861.752432644367 total_train_data : 7014\n",
      "Epoch 138 Loss : 4861.752438247204 total_train_data : 7014\n",
      "Epoch 139 Loss : 4861.752441108227 total_train_data : 7014\n",
      "Epoch 140 Loss : 4861.75244474411 total_train_data : 7014\n",
      "Epoch 141 Loss : 4861.752439916134 total_train_data : 7014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 142 Loss : 4861.752418458462 total_train_data : 7014\n",
      "Epoch 143 Loss : 4861.7524127960205 total_train_data : 7014\n",
      "Epoch 144 Loss : 4861.752415299416 total_train_data : 7014\n",
      "Epoch 145 Loss : 4861.752420127392 total_train_data : 7014\n",
      "Epoch 146 Loss : 4861.752434015274 total_train_data : 7014\n",
      "Epoch 147 Loss : 4861.752408087254 total_train_data : 7014\n",
      "Epoch 148 Loss : 4861.752416729927 total_train_data : 7014\n",
      "Epoch 149 Loss : 4861.752417743206 total_train_data : 7014\n",
      "Epoch 150 Loss : 4861.752416908741 total_train_data : 7014\n",
      "Epoch 151 Loss : 4861.75243639946 total_train_data : 7014\n",
      "Epoch 152 Loss : 4861.752413213253 total_train_data : 7014\n",
      "Epoch 153 Loss : 4861.752420127392 total_train_data : 7014\n",
      "Epoch 154 Loss : 4861.752407610416 total_train_data : 7014\n",
      "Epoch 155 Loss : 4861.752406656742 total_train_data : 7014\n",
      "Epoch 156 Loss : 4861.752424061298 total_train_data : 7014\n",
      "Epoch 157 Loss : 4861.752394735813 total_train_data : 7014\n",
      "Epoch 158 Loss : 4861.752418100834 total_train_data : 7014\n",
      "Epoch 159 Loss : 4861.752425670624 total_train_data : 7014\n",
      "Epoch 160 Loss : 4861.75241458416 total_train_data : 7014\n",
      "Epoch 161 Loss : 4861.7524029016495 total_train_data : 7014\n",
      "Epoch 162 Loss : 4861.752413988113 total_train_data : 7014\n",
      "Epoch 163 Loss : 4861.752412080765 total_train_data : 7014\n",
      "Epoch 164 Loss : 4861.752412974834 total_train_data : 7014\n",
      "Epoch 165 Loss : 4861.752412736416 total_train_data : 7014\n",
      "Epoch 166 Loss : 4861.7524201869965 total_train_data : 7014\n",
      "Epoch 167 Loss : 4861.752406597137 total_train_data : 7014\n",
      "Epoch 168 Loss : 4861.752411723137 total_train_data : 7014\n",
      "Epoch 169 Loss : 4861.752420306206 total_train_data : 7014\n",
      "Epoch 170 Loss : 4861.752422511578 total_train_data : 7014\n",
      "Epoch 171 Loss : 4861.752404689789 total_train_data : 7014\n",
      "Epoch 172 Loss : 4861.752402603626 total_train_data : 7014\n",
      "Epoch 173 Loss : 4861.752402544022 total_train_data : 7014\n",
      "Epoch 174 Loss : 4861.752400815487 total_train_data : 7014\n",
      "Epoch 175 Loss : 4861.752408206463 total_train_data : 7014\n",
      "Epoch 176 Loss : 4861.75242036581 total_train_data : 7014\n",
      "Epoch 177 Loss : 4861.752409517765 total_train_data : 7014\n",
      "Epoch 178 Loss : 4861.752413034439 total_train_data : 7014\n",
      "Epoch 179 Loss : 4861.752402305603 total_train_data : 7014\n",
      "Epoch 180 Loss : 4861.75239187479 total_train_data : 7014\n",
      "Epoch 181 Loss : 4861.752399563789 total_train_data : 7014\n",
      "Epoch 182 Loss : 4861.752400755882 total_train_data : 7014\n",
      "Epoch 183 Loss : 4861.752400159836 total_train_data : 7014\n",
      "Epoch 184 Loss : 4861.752413988113 total_train_data : 7014\n",
      "Epoch 185 Loss : 4861.7524154782295 total_train_data : 7014\n",
      "Epoch 186 Loss : 4861.752408564091 total_train_data : 7014\n",
      "Epoch 187 Loss : 4861.752400934696 total_train_data : 7014\n",
      "Epoch 188 Loss : 4861.752393126488 total_train_data : 7014\n",
      "Epoch 189 Loss : 4861.752392947674 total_train_data : 7014\n",
      "Epoch 190 Loss : 4861.7523956894875 total_train_data : 7014\n",
      "Epoch 191 Loss : 4861.75238853693 total_train_data : 7014\n",
      "Epoch 192 Loss : 4861.752398312092 total_train_data : 7014\n",
      "Epoch 193 Loss : 4861.752415776253 total_train_data : 7014\n",
      "Epoch 194 Loss : 4861.752396225929 total_train_data : 7014\n",
      "Epoch 195 Loss : 4861.752392590046 total_train_data : 7014\n",
      "Epoch 196 Loss : 4861.752386689186 total_train_data : 7014\n",
      "Epoch 197 Loss : 4861.752385020256 total_train_data : 7014\n",
      "Epoch 198 Loss : 4861.752379834652 total_train_data : 7014\n",
      "Epoch 199 Loss : 4861.752408146858 total_train_data : 7014\n",
      "Epoch 200 Loss : 4861.752397060394 total_train_data : 7014\n",
      "Epoch 201 Loss : 4861.752397418022 total_train_data : 7014\n",
      "Epoch 202 Loss : 4861.752381145954 total_train_data : 7014\n",
      "Epoch 203 Loss : 4861.752379477024 total_train_data : 7014\n",
      "Epoch 204 Loss : 4861.752397537231 total_train_data : 7014\n",
      "Epoch 205 Loss : 4861.752393484116 total_train_data : 7014\n",
      "Epoch 206 Loss : 4861.752400636673 total_train_data : 7014\n",
      "Epoch 207 Loss : 4861.7523956894875 total_train_data : 7014\n",
      "Epoch 208 Loss : 4861.752402484417 total_train_data : 7014\n",
      "Epoch 209 Loss : 4861.752386927605 total_train_data : 7014\n",
      "Epoch 210 Loss : 4861.75238096714 total_train_data : 7014\n",
      "Epoch 211 Loss : 4861.752367198467 total_train_data : 7014\n",
      "Epoch 212 Loss : 4861.752389013767 total_train_data : 7014\n",
      "Epoch 213 Loss : 4861.752384841442 total_train_data : 7014\n",
      "Epoch 214 Loss : 4861.752397954464 total_train_data : 7014\n",
      "Epoch 215 Loss : 4861.752394258976 total_train_data : 7014\n",
      "Epoch 216 Loss : 4861.7523983716965 total_train_data : 7014\n",
      "Epoch 217 Loss : 4861.752379953861 total_train_data : 7014\n",
      "Epoch 218 Loss : 4861.752375602722 total_train_data : 7014\n",
      "Epoch 219 Loss : 4861.7523638010025 total_train_data : 7014\n",
      "Epoch 220 Loss : 4861.75238096714 total_train_data : 7014\n",
      "Epoch 221 Loss : 4861.752379834652 total_train_data : 7014\n",
      "Epoch 222 Loss : 4861.752395868301 total_train_data : 7014\n",
      "Epoch 223 Loss : 4861.75236582756 total_train_data : 7014\n",
      "Epoch 224 Loss : 4861.752397954464 total_train_data : 7014\n",
      "Epoch 225 Loss : 4861.752370655537 total_train_data : 7014\n",
      "Epoch 226 Loss : 4861.752389490604 total_train_data : 7014\n",
      "Epoch 227 Loss : 4861.752383112907 total_train_data : 7014\n",
      "Epoch 228 Loss : 4861.752383291721 total_train_data : 7014\n",
      "Epoch 229 Loss : 4861.752389192581 total_train_data : 7014\n",
      "Epoch 230 Loss : 4861.75236338377 total_train_data : 7014\n",
      "Epoch 231 Loss : 4861.752396821976 total_train_data : 7014\n",
      "Epoch 232 Loss : 4861.752366602421 total_train_data : 7014\n",
      "Epoch 233 Loss : 4861.75239109993 total_train_data : 7014\n",
      "Epoch 234 Loss : 4861.752366662025 total_train_data : 7014\n",
      "Epoch 235 Loss : 4861.752384066582 total_train_data : 7014\n",
      "Epoch 236 Loss : 4861.752381742001 total_train_data : 7014\n",
      "Epoch 237 Loss : 4861.75235337019 total_train_data : 7014\n",
      "Epoch 238 Loss : 4861.7523882985115 total_train_data : 7014\n",
      "Epoch 239 Loss : 4861.752359032631 total_train_data : 7014\n",
      "Epoch 240 Loss : 4861.752397239208 total_train_data : 7014\n",
      "Epoch 241 Loss : 4861.752363145351 total_train_data : 7014\n",
      "Epoch 242 Loss : 4861.752376616001 total_train_data : 7014\n",
      "Epoch 243 Loss : 4861.752367734909 total_train_data : 7014\n",
      "Epoch 244 Loss : 4861.752371251583 total_train_data : 7014\n",
      "Epoch 245 Loss : 4861.752383887768 total_train_data : 7014\n",
      "Epoch 246 Loss : 4861.752356529236 total_train_data : 7014\n",
      "Epoch 247 Loss : 4861.7523893117905 total_train_data : 7014\n",
      "Epoch 248 Loss : 4861.752365469933 total_train_data : 7014\n",
      "Epoch 249 Loss : 4861.752391159534 total_train_data : 7014\n",
      "Epoch 250 Loss : 4861.752360343933 total_train_data : 7014\n",
      "Epoch 251 Loss : 4861.752361416817 total_train_data : 7014\n",
      "Epoch 252 Loss : 4861.7523901462555 total_train_data : 7014\n",
      "Epoch 253 Loss : 4861.75236082077 total_train_data : 7014\n",
      "Epoch 254 Loss : 4861.752362728119 total_train_data : 7014\n",
      "Epoch 255 Loss : 4861.752381265163 total_train_data : 7014\n",
      "Epoch 256 Loss : 4861.752386510372 total_train_data : 7014\n",
      "Epoch 257 Loss : 4861.752353966236 total_train_data : 7014\n",
      "Epoch 258 Loss : 4861.752366781235 total_train_data : 7014\n",
      "Epoch 259 Loss : 4861.752379655838 total_train_data : 7014\n",
      "Epoch 260 Loss : 4861.752368450165 total_train_data : 7014\n",
      "Epoch 261 Loss : 4861.752356350422 total_train_data : 7014\n",
      "Epoch 262 Loss : 4861.752382159233 total_train_data : 7014\n",
      "Epoch 263 Loss : 4861.752356290817 total_train_data : 7014\n",
      "Epoch 264 Loss : 4861.752364933491 total_train_data : 7014\n",
      "Epoch 265 Loss : 4861.752396643162 total_train_data : 7014\n",
      "Epoch 266 Loss : 4861.75235748291 total_train_data : 7014\n",
      "Epoch 267 Loss : 4861.752355277538 total_train_data : 7014\n",
      "Epoch 268 Loss : 4861.752382397652 total_train_data : 7014\n",
      "Epoch 269 Loss : 4861.752352654934 total_train_data : 7014\n",
      "Epoch 270 Loss : 4861.752350270748 total_train_data : 7014\n",
      "Epoch 271 Loss : 4861.752382516861 total_train_data : 7014\n",
      "Epoch 272 Loss : 4861.752360522747 total_train_data : 7014\n",
      "Epoch 273 Loss : 4861.752354383469 total_train_data : 7014\n",
      "Epoch 274 Loss : 4861.752359390259 total_train_data : 7014\n",
      "Epoch 275 Loss : 4861.752374351025 total_train_data : 7014\n",
      "Epoch 276 Loss : 4861.752347946167 total_train_data : 7014\n",
      "Epoch 277 Loss : 4861.752348661423 total_train_data : 7014\n",
      "Epoch 278 Loss : 4861.752382397652 total_train_data : 7014\n",
      "Epoch 279 Loss : 4861.752359867096 total_train_data : 7014\n",
      "Epoch 280 Loss : 4861.752348482609 total_train_data : 7014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 281 Loss : 4861.752377748489 total_train_data : 7014\n",
      "Epoch 282 Loss : 4861.752375841141 total_train_data : 7014\n",
      "Epoch 283 Loss : 4861.752376437187 total_train_data : 7014\n",
      "Epoch 284 Loss : 4861.752377688885 total_train_data : 7014\n",
      "Epoch 285 Loss : 4861.752377450466 total_train_data : 7014\n",
      "Epoch 286 Loss : 4861.752377450466 total_train_data : 7014\n",
      "Epoch 287 Loss : 4861.752377450466 total_train_data : 7014\n",
      "Epoch 288 Loss : 4861.752377450466 total_train_data : 7014\n",
      "Epoch 289 Loss : 4861.752377450466 total_train_data : 7014\n",
      "Epoch 290 Loss : 4861.752377450466 total_train_data : 7014\n",
      "Epoch 291 Loss : 4861.752377450466 total_train_data : 7014\n",
      "Epoch 292 Loss : 4861.75237518549 total_train_data : 7014\n",
      "Epoch 293 Loss : 4861.75237518549 total_train_data : 7014\n",
      "Epoch 294 Loss : 4861.75237518549 total_train_data : 7014\n",
      "Epoch 295 Loss : 4861.752372503281 total_train_data : 7014\n",
      "Epoch 296 Loss : 4861.75237518549 total_train_data : 7014\n",
      "Epoch 297 Loss : 4861.75237518549 total_train_data : 7014\n",
      "Epoch 298 Loss : 4861.75237518549 total_train_data : 7014\n",
      "Epoch 299 Loss : 4861.75237518549 total_train_data : 7014\n",
      "Epoch 300 Loss : 4861.752375125885 total_train_data : 7014\n",
      "Epoch 301 Loss : 4861.752370595932 total_train_data : 7014\n",
      "Epoch 302 Loss : 4861.752370595932 total_train_data : 7014\n",
      "Epoch 303 Loss : 4861.752370595932 total_train_data : 7014\n",
      "Epoch 304 Loss : 4861.752370595932 total_train_data : 7014\n",
      "Epoch 305 Loss : 4861.752369821072 total_train_data : 7014\n",
      "Epoch 306 Loss : 4861.752369761467 total_train_data : 7014\n",
      "Epoch 307 Loss : 4861.752369761467 total_train_data : 7014\n",
      "Epoch 308 Loss : 4861.752369761467 total_train_data : 7014\n",
      "Epoch 309 Loss : 4861.752369642258 total_train_data : 7014\n",
      "Epoch 310 Loss : 4861.7523691654205 total_train_data : 7014\n",
      "Epoch 311 Loss : 4861.752364575863 total_train_data : 7014\n",
      "Epoch 312 Loss : 4861.752364575863 total_train_data : 7014\n",
      "Epoch 313 Loss : 4861.752364575863 total_train_data : 7014\n",
      "Epoch 314 Loss : 4861.752364575863 total_train_data : 7014\n",
      "Epoch 315 Loss : 4861.752364575863 total_train_data : 7014\n",
      "Epoch 316 Loss : 4861.752364575863 total_train_data : 7014\n",
      "Epoch 317 Loss : 4861.752364575863 total_train_data : 7014\n",
      "Epoch 318 Loss : 4861.752364575863 total_train_data : 7014\n",
      "Epoch 319 Loss : 4861.752371668816 total_train_data : 7014\n",
      "Epoch 320 Loss : 4861.752371668816 total_train_data : 7014\n",
      "Epoch 321 Loss : 4861.752372384071 total_train_data : 7014\n",
      "Epoch 322 Loss : 4861.752365171909 total_train_data : 7014\n",
      "Epoch 323 Loss : 4861.75235003233 total_train_data : 7014\n",
      "Epoch 324 Loss : 4861.752379953861 total_train_data : 7014\n",
      "Epoch 325 Loss : 4861.7523475289345 total_train_data : 7014\n",
      "Epoch 326 Loss : 4861.752378821373 total_train_data : 7014\n",
      "Epoch 327 Loss : 4861.7523475289345 total_train_data : 7014\n",
      "Epoch 328 Loss : 4861.752379179001 total_train_data : 7014\n",
      "Epoch 329 Loss : 4861.752349555492 total_train_data : 7014\n",
      "Epoch 330 Loss : 4861.752380788326 total_train_data : 7014\n",
      "Epoch 331 Loss : 4861.752349555492 total_train_data : 7014\n",
      "Epoch 332 Loss : 4861.752380788326 total_train_data : 7014\n",
      "Epoch 333 Loss : 4861.752349555492 total_train_data : 7014\n",
      "Epoch 334 Loss : 4861.752380788326 total_train_data : 7014\n",
      "Epoch 335 Loss : 4861.752349555492 total_train_data : 7014\n",
      "Epoch 336 Loss : 4861.752380788326 total_train_data : 7014\n",
      "Epoch 337 Loss : 4861.752349555492 total_train_data : 7014\n",
      "Epoch 338 Loss : 4861.752380788326 total_train_data : 7014\n",
      "Epoch 339 Loss : 4861.752349555492 total_train_data : 7014\n",
      "Epoch 340 Loss : 4861.752380788326 total_train_data : 7014\n",
      "Epoch 341 Loss : 4861.7523493766785 total_train_data : 7014\n",
      "Epoch 342 Loss : 4861.752373993397 total_train_data : 7014\n",
      "Epoch 343 Loss : 4861.7523493766785 total_train_data : 7014\n",
      "Epoch 344 Loss : 4861.752373993397 total_train_data : 7014\n",
      "Epoch 345 Loss : 4861.752342700958 total_train_data : 7014\n",
      "Epoch 346 Loss : 4861.752373993397 total_train_data : 7014\n",
      "Epoch 347 Loss : 4861.752342641354 total_train_data : 7014\n",
      "Epoch 348 Loss : 4861.752373933792 total_train_data : 7014\n",
      "Epoch 349 Loss : 4861.752342641354 total_train_data : 7014\n",
      "Epoch 350 Loss : 4861.752373933792 total_train_data : 7014\n",
      "Epoch 351 Loss : 4861.752342641354 total_train_data : 7014\n",
      "Epoch 352 Loss : 4861.752373933792 total_train_data : 7014\n",
      "Epoch 353 Loss : 4861.752336442471 total_train_data : 7014\n",
      "Epoch 354 Loss : 4861.752376079559 total_train_data : 7014\n",
      "Epoch 355 Loss : 4861.752336442471 total_train_data : 7014\n",
      "Epoch 356 Loss : 4861.752376079559 total_train_data : 7014\n",
      "Epoch 357 Loss : 4861.752336442471 total_train_data : 7014\n",
      "Epoch 358 Loss : 4861.752376079559 total_train_data : 7014\n",
      "Epoch 359 Loss : 4861.752336442471 total_train_data : 7014\n",
      "Epoch 360 Loss : 4861.752376079559 total_train_data : 7014\n",
      "Epoch 361 Loss : 4861.752336442471 total_train_data : 7014\n",
      "Epoch 362 Loss : 4861.752375245094 total_train_data : 7014\n",
      "Epoch 363 Loss : 4861.752330601215 total_train_data : 7014\n",
      "Epoch 364 Loss : 4861.752374589443 total_train_data : 7014\n",
      "Epoch 365 Loss : 4861.752330601215 total_train_data : 7014\n",
      "Epoch 366 Loss : 4861.752374589443 total_train_data : 7014\n",
      "Epoch 367 Loss : 4861.752330601215 total_train_data : 7014\n",
      "Epoch 368 Loss : 4861.752374589443 total_train_data : 7014\n",
      "Epoch 369 Loss : 4861.752330601215 total_train_data : 7014\n",
      "Epoch 370 Loss : 4861.752374589443 total_train_data : 7014\n",
      "Epoch 371 Loss : 4861.752330601215 total_train_data : 7014\n",
      "Epoch 372 Loss : 4861.752374589443 total_train_data : 7014\n",
      "Epoch 373 Loss : 4861.752330601215 total_train_data : 7014\n",
      "Epoch 374 Loss : 4861.752367556095 total_train_data : 7014\n",
      "Epoch 375 Loss : 4861.752329647541 total_train_data : 7014\n",
      "Epoch 376 Loss : 4861.752367556095 total_train_data : 7014\n",
      "Epoch 377 Loss : 4861.752329647541 total_train_data : 7014\n",
      "Epoch 378 Loss : 4861.752367556095 total_train_data : 7014\n",
      "Epoch 379 Loss : 4861.752329647541 total_train_data : 7014\n",
      "Epoch 380 Loss : 4861.752367556095 total_train_data : 7014\n",
      "Epoch 381 Loss : 4861.752329647541 total_train_data : 7014\n",
      "Epoch 382 Loss : 4861.752367556095 total_train_data : 7014\n",
      "Epoch 383 Loss : 4861.752329647541 total_train_data : 7014\n",
      "Epoch 384 Loss : 4861.752367556095 total_train_data : 7014\n",
      "Epoch 385 Loss : 4861.752329647541 total_train_data : 7014\n",
      "Epoch 386 Loss : 4861.752367556095 total_train_data : 7014\n",
      "Epoch 387 Loss : 4861.752329289913 total_train_data : 7014\n",
      "Epoch 388 Loss : 4861.752365887165 total_train_data : 7014\n",
      "Epoch 389 Loss : 4861.752329289913 total_train_data : 7014\n",
      "Epoch 390 Loss : 4861.752365887165 total_train_data : 7014\n",
      "Epoch 391 Loss : 4861.752329289913 total_train_data : 7014\n",
      "Epoch 392 Loss : 4861.752365887165 total_train_data : 7014\n",
      "Epoch 393 Loss : 4861.752329289913 total_train_data : 7014\n",
      "Epoch 394 Loss : 4861.752365887165 total_train_data : 7014\n",
      "Epoch 395 Loss : 4861.752329289913 total_train_data : 7014\n",
      "Epoch 396 Loss : 4861.752365887165 total_train_data : 7014\n",
      "Epoch 397 Loss : 4861.752329289913 total_train_data : 7014\n",
      "Epoch 398 Loss : 4861.752365887165 total_train_data : 7014\n",
      "Epoch 399 Loss : 4861.75233566761 total_train_data : 7014\n",
      "Epoch 400 Loss : 4861.752364516258 total_train_data : 7014\n",
      "Epoch 401 Loss : 4861.75233566761 total_train_data : 7014\n",
      "Epoch 402 Loss : 4861.752364516258 total_train_data : 7014\n",
      "Epoch 403 Loss : 4861.7523356080055 total_train_data : 7014\n",
      "Epoch 404 Loss : 4861.752364516258 total_train_data : 7014\n",
      "Epoch 405 Loss : 4861.752338051796 total_train_data : 7014\n",
      "Epoch 406 Loss : 4861.752356946468 total_train_data : 7014\n",
      "Epoch 407 Loss : 4861.752338171005 total_train_data : 7014\n",
      "Epoch 408 Loss : 4861.752346217632 total_train_data : 7014\n",
      "Epoch 409 Loss : 4861.752329349518 total_train_data : 7014\n",
      "Epoch 410 Loss : 4861.7523601055145 total_train_data : 7014\n",
      "Epoch 411 Loss : 4861.752356231213 total_train_data : 7014\n",
      "Epoch 412 Loss : 4861.752336740494 total_train_data : 7014\n",
      "Epoch 413 Loss : 4861.752334237099 total_train_data : 7014\n",
      "Epoch 414 Loss : 4861.752327144146 total_train_data : 7014\n",
      "Epoch 415 Loss : 4861.752345144749 total_train_data : 7014\n",
      "Epoch 416 Loss : 4861.752365529537 total_train_data : 7014\n",
      "Epoch 417 Loss : 4861.752348721027 total_train_data : 7014\n",
      "Epoch 418 Loss : 4861.752341866493 total_train_data : 7014\n",
      "Epoch 419 Loss : 4861.75231975317 total_train_data : 7014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 420 Loss : 4861.752335131168 total_train_data : 7014\n",
      "Epoch 421 Loss : 4861.752363562584 total_train_data : 7014\n",
      "Epoch 422 Loss : 4861.75235170126 total_train_data : 7014\n",
      "Epoch 423 Loss : 4861.7523357868195 total_train_data : 7014\n",
      "Epoch 424 Loss : 4861.75232988596 total_train_data : 7014\n",
      "Epoch 425 Loss : 4861.752321124077 total_train_data : 7014\n",
      "Epoch 426 Loss : 4861.752361595631 total_train_data : 7014\n",
      "Epoch 427 Loss : 4861.752358019352 total_train_data : 7014\n",
      "Epoch 428 Loss : 4861.752336680889 total_train_data : 7014\n",
      "Epoch 429 Loss : 4861.752335071564 total_train_data : 7014\n",
      "Epoch 430 Loss : 4861.752328038216 total_train_data : 7014\n",
      "Epoch 431 Loss : 4861.752347290516 total_train_data : 7014\n",
      "Epoch 432 Loss : 4861.752367019653 total_train_data : 7014\n",
      "Epoch 433 Loss : 4861.75235003233 total_train_data : 7014\n",
      "Epoch 434 Loss : 4861.7523428201675 total_train_data : 7014\n",
      "Epoch 435 Loss : 4861.752321600914 total_train_data : 7014\n",
      "Epoch 436 Loss : 4861.752336978912 total_train_data : 7014\n",
      "Epoch 437 Loss : 4861.752360463142 total_train_data : 7014\n",
      "Epoch 438 Loss : 4861.75235170126 total_train_data : 7014\n",
      "Epoch 439 Loss : 4861.7523357868195 total_train_data : 7014\n",
      "Epoch 440 Loss : 4861.75232988596 total_train_data : 7014\n",
      "Epoch 441 Loss : 4861.752321124077 total_train_data : 7014\n",
      "Epoch 442 Loss : 4861.752360999584 total_train_data : 7014\n",
      "Epoch 443 Loss : 4861.752356767654 total_train_data : 7014\n",
      "Epoch 444 Loss : 4861.752336680889 total_train_data : 7014\n",
      "Epoch 445 Loss : 4861.752335071564 total_train_data : 7014\n",
      "Epoch 446 Loss : 4861.752328038216 total_train_data : 7014\n",
      "Epoch 447 Loss : 4861.752347290516 total_train_data : 7014\n",
      "Epoch 448 Loss : 4861.752367019653 total_train_data : 7014\n",
      "Epoch 449 Loss : 4861.75235003233 total_train_data : 7014\n",
      "Epoch 450 Loss : 4861.7523428201675 total_train_data : 7014\n",
      "Epoch 451 Loss : 4861.752321600914 total_train_data : 7014\n",
      "Epoch 452 Loss : 4861.752336978912 total_train_data : 7014\n",
      "Epoch 453 Loss : 4861.752360463142 total_train_data : 7014\n",
      "Epoch 454 Loss : 4861.752351105213 total_train_data : 7014\n",
      "Epoch 455 Loss : 4861.752334535122 total_train_data : 7014\n",
      "Epoch 456 Loss : 4861.75232988596 total_train_data : 7014\n",
      "Epoch 457 Loss : 4861.752318024635 total_train_data : 7014\n",
      "Epoch 458 Loss : 4861.752354383469 total_train_data : 7014\n",
      "Epoch 459 Loss : 4861.752356767654 total_train_data : 7014\n",
      "Epoch 460 Loss : 4861.752336680889 total_train_data : 7014\n",
      "Epoch 461 Loss : 4861.752331972122 total_train_data : 7014\n",
      "Epoch 462 Loss : 4861.7523214221 total_train_data : 7014\n",
      "Epoch 463 Loss : 4861.752346932888 total_train_data : 7014\n",
      "Epoch 464 Loss : 4861.752366840839 total_train_data : 7014\n",
      "Epoch 465 Loss : 4861.75235003233 total_train_data : 7014\n",
      "Epoch 466 Loss : 4861.7523428201675 total_train_data : 7014\n",
      "Epoch 467 Loss : 4861.752321541309 total_train_data : 7014\n",
      "Epoch 468 Loss : 4861.752336800098 total_train_data : 7014\n",
      "Epoch 469 Loss : 4861.752360463142 total_train_data : 7014\n",
      "Epoch 470 Loss : 4861.752344369888 total_train_data : 7014\n",
      "Epoch 471 Loss : 4861.752334415913 total_train_data : 7014\n",
      "Epoch 472 Loss : 4861.75232988596 total_train_data : 7014\n",
      "Epoch 473 Loss : 4861.752318024635 total_train_data : 7014\n",
      "Epoch 474 Loss : 4861.752354383469 total_train_data : 7014\n",
      "Epoch 475 Loss : 4861.75235670805 total_train_data : 7014\n",
      "Epoch 476 Loss : 4861.752336680889 total_train_data : 7014\n",
      "Epoch 477 Loss : 4861.752331912518 total_train_data : 7014\n",
      "Epoch 478 Loss : 4861.7523214221 total_train_data : 7014\n",
      "Epoch 479 Loss : 4861.752346932888 total_train_data : 7014\n",
      "Epoch 480 Loss : 4861.752366840839 total_train_data : 7014\n",
      "Epoch 481 Loss : 4861.752349972725 total_train_data : 7014\n",
      "Epoch 482 Loss : 4861.752342700958 total_train_data : 7014\n",
      "Epoch 483 Loss : 4861.752321541309 total_train_data : 7014\n",
      "Epoch 484 Loss : 4861.752336800098 total_train_data : 7014\n",
      "Epoch 485 Loss : 4861.752361297607 total_train_data : 7014\n",
      "Epoch 486 Loss : 4861.752346456051 total_train_data : 7014\n",
      "Epoch 487 Loss : 4861.752335727215 total_train_data : 7014\n",
      "Epoch 488 Loss : 4861.752332031727 total_train_data : 7014\n",
      "Epoch 489 Loss : 4861.752321302891 total_train_data : 7014\n",
      "Epoch 490 Loss : 4861.752356529236 total_train_data : 7014\n",
      "Epoch 491 Loss : 4861.752351462841 total_train_data : 7014\n",
      "Epoch 492 Loss : 4861.75233066082 total_train_data : 7014\n",
      "Epoch 493 Loss : 4861.752317428589 total_train_data : 7014\n",
      "Epoch 494 Loss : 4861.752312541008 total_train_data : 7014\n",
      "Epoch 495 Loss : 4861.75234746933 total_train_data : 7014\n",
      "Epoch 496 Loss : 4861.752359688282 total_train_data : 7014\n",
      "Epoch 497 Loss : 4861.752337157726 total_train_data : 7014\n",
      "Epoch 498 Loss : 4861.752328634262 total_train_data : 7014\n",
      "Epoch 499 Loss : 4861.752322494984 total_train_data : 7014\n",
      "Epoch 500 Loss : 4861.752337813377 total_train_data : 7014\n",
      "Epoch 501 Loss : 4861.752361297607 total_train_data : 7014\n",
      "Epoch 502 Loss : 4861.752344727516 total_train_data : 7014\n",
      "Epoch 503 Loss : 4861.752335369587 total_train_data : 7014\n",
      "Epoch 504 Loss : 4861.752332031727 total_train_data : 7014\n",
      "Epoch 505 Loss : 4861.752321302891 total_train_data : 7014\n",
      "Epoch 506 Loss : 4861.752356290817 total_train_data : 7014\n",
      "Epoch 507 Loss : 4861.752344071865 total_train_data : 7014\n",
      "Epoch 508 Loss : 4861.75233066082 total_train_data : 7014\n",
      "Epoch 509 Loss : 4861.752317428589 total_train_data : 7014\n",
      "Epoch 510 Loss : 4861.752312421799 total_train_data : 7014\n",
      "Epoch 511 Loss : 4861.752343773842 total_train_data : 7014\n",
      "Epoch 512 Loss : 4861.752359688282 total_train_data : 7014\n",
      "Epoch 513 Loss : 4861.752336144447 total_train_data : 7014\n",
      "Epoch 514 Loss : 4861.752324521542 total_train_data : 7014\n",
      "Epoch 515 Loss : 4861.752317786217 total_train_data : 7014\n",
      "Epoch 516 Loss : 4861.752336442471 total_train_data : 7014\n",
      "Epoch 517 Loss : 4861.752359032631 total_train_data : 7014\n",
      "Epoch 518 Loss : 4861.752338349819 total_train_data : 7014\n",
      "Epoch 519 Loss : 4861.752335011959 total_train_data : 7014\n",
      "Epoch 520 Loss : 4861.752329468727 total_train_data : 7014\n",
      "Epoch 521 Loss : 4861.75232052803 total_train_data : 7014\n",
      "Epoch 522 Loss : 4861.752356231213 total_train_data : 7014\n",
      "Epoch 523 Loss : 4861.752336859703 total_train_data : 7014\n",
      "Epoch 524 Loss : 4861.75233066082 total_train_data : 7014\n",
      "Epoch 525 Loss : 4861.752317368984 total_train_data : 7014\n",
      "Epoch 526 Loss : 4861.752312421799 total_train_data : 7014\n",
      "Epoch 527 Loss : 4861.752343773842 total_train_data : 7014\n",
      "Epoch 528 Loss : 4861.752357244492 total_train_data : 7014\n",
      "Epoch 529 Loss : 4861.752336084843 total_train_data : 7014\n",
      "Epoch 530 Loss : 4861.752324402332 total_train_data : 7014\n",
      "Epoch 531 Loss : 4861.752317547798 total_train_data : 7014\n",
      "Epoch 532 Loss : 4861.7523339390755 total_train_data : 7014\n",
      "Epoch 533 Loss : 4861.752351522446 total_train_data : 7014\n",
      "Epoch 534 Loss : 4861.752338349819 total_train_data : 7014\n",
      "Epoch 535 Loss : 4861.752335011959 total_train_data : 7014\n",
      "Epoch 536 Loss : 4861.752329468727 total_train_data : 7014\n",
      "Epoch 537 Loss : 4861.75232052803 total_train_data : 7014\n",
      "Epoch 538 Loss : 4861.752356231213 total_train_data : 7014\n",
      "Epoch 539 Loss : 4861.752336859703 total_train_data : 7014\n",
      "Epoch 540 Loss : 4861.752330124378 total_train_data : 7014\n",
      "Epoch 541 Loss : 4861.752315998077 total_train_data : 7014\n",
      "Epoch 542 Loss : 4861.752312421799 total_train_data : 7014\n",
      "Epoch 543 Loss : 4861.752344787121 total_train_data : 7014\n",
      "Epoch 544 Loss : 4861.752357125282 total_train_data : 7014\n",
      "Epoch 545 Loss : 4861.752325832844 total_train_data : 7014\n",
      "Epoch 546 Loss : 4861.752323389053 total_train_data : 7014\n",
      "Epoch 547 Loss : 4861.752316534519 total_train_data : 7014\n",
      "Epoch 548 Loss : 4861.752334356308 total_train_data : 7014\n",
      "Epoch 549 Loss : 4861.752352535725 total_train_data : 7014\n",
      "Epoch 550 Loss : 4861.752336919308 total_train_data : 7014\n",
      "Epoch 551 Loss : 4861.752335011959 total_train_data : 7014\n",
      "Epoch 552 Loss : 4861.752327978611 total_train_data : 7014\n",
      "Epoch 553 Loss : 4861.7523156404495 total_train_data : 7014\n",
      "Epoch 554 Loss : 4861.752349555492 total_train_data : 7014\n",
      "Epoch 555 Loss : 4861.752336859703 total_train_data : 7014\n",
      "Epoch 556 Loss : 4861.752331733704 total_train_data : 7014\n",
      "Epoch 557 Loss : 4861.752314984798 total_train_data : 7014\n",
      "Epoch 558 Loss : 4861.752304136753 total_train_data : 7014\n",
      "Epoch 559 Loss : 4861.752344787121 total_train_data : 7014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 560 Loss : 4861.752357125282 total_train_data : 7014\n",
      "Epoch 561 Loss : 4861.752325832844 total_train_data : 7014\n",
      "Epoch 562 Loss : 4861.752323389053 total_train_data : 7014\n",
      "Epoch 563 Loss : 4861.752316534519 total_train_data : 7014\n",
      "Epoch 564 Loss : 4861.752334356308 total_train_data : 7014\n",
      "Epoch 565 Loss : 4861.752352535725 total_train_data : 7014\n",
      "Epoch 566 Loss : 4861.752336919308 total_train_data : 7014\n",
      "Epoch 567 Loss : 4861.752335011959 total_train_data : 7014\n",
      "Epoch 568 Loss : 4861.752326846123 total_train_data : 7014\n",
      "Epoch 569 Loss : 4861.752315342426 total_train_data : 7014\n",
      "Epoch 570 Loss : 4861.752349555492 total_train_data : 7014\n",
      "Epoch 571 Loss : 4861.752336859703 total_train_data : 7014\n",
      "Epoch 572 Loss : 4861.75232899189 total_train_data : 7014\n",
      "Epoch 573 Loss : 4861.752314150333 total_train_data : 7014\n",
      "Epoch 574 Loss : 4861.752304136753 total_train_data : 7014\n",
      "Epoch 575 Loss : 4861.752344787121 total_train_data : 7014\n",
      "Epoch 576 Loss : 4861.752354443073 total_train_data : 7014\n",
      "Epoch 577 Loss : 4861.752322793007 total_train_data : 7014\n",
      "Epoch 578 Loss : 4861.752323389053 total_train_data : 7014\n",
      "Epoch 579 Loss : 4861.752316534519 total_train_data : 7014\n",
      "Epoch 580 Loss : 4861.752331912518 total_train_data : 7014\n",
      "Epoch 581 Loss : 4861.75235080719 total_train_data : 7014\n",
      "Epoch 582 Loss : 4861.752335369587 total_train_data : 7014\n",
      "Epoch 583 Loss : 4861.752335011959 total_train_data : 7014\n",
      "Epoch 584 Loss : 4861.752324402332 total_train_data : 7014\n",
      "Epoch 585 Loss : 4861.752308249474 total_train_data : 7014\n",
      "Epoch 586 Loss : 4861.752347111702 total_train_data : 7014\n",
      "Epoch 587 Loss : 4861.752335965633 total_train_data : 7014\n",
      "Epoch 588 Loss : 4861.752328753471 total_train_data : 7014\n",
      "Epoch 589 Loss : 4861.752310872078 total_train_data : 7014\n",
      "Epoch 590 Loss : 4861.752304136753 total_train_data : 7014\n",
      "Epoch 591 Loss : 4861.752339839935 total_train_data : 7014\n",
      "Epoch 592 Loss : 4861.752353966236 total_train_data : 7014\n",
      "Epoch 593 Loss : 4861.752322793007 total_train_data : 7014\n",
      "Epoch 594 Loss : 4861.752323389053 total_train_data : 7014\n",
      "Epoch 595 Loss : 4861.752313375473 total_train_data : 7014\n",
      "Epoch 596 Loss : 4861.752331912518 total_train_data : 7014\n",
      "Epoch 597 Loss : 4861.75235080719 total_train_data : 7014\n",
      "Epoch 598 Loss : 4861.752335369587 total_train_data : 7014\n",
      "Epoch 599 Loss : 4861.752335011959 total_train_data : 7014\n",
      "Epoch 600 Loss : 4861.752320587635 total_train_data : 7014\n",
      "Epoch 601 Loss : 4861.752308249474 total_train_data : 7014\n",
      "Epoch 602 Loss : 4861.752347111702 total_train_data : 7014\n",
      "Epoch 603 Loss : 4861.752335965633 total_train_data : 7014\n",
      "Epoch 604 Loss : 4861.752326846123 total_train_data : 7014\n",
      "Epoch 605 Loss : 4861.752310872078 total_train_data : 7014\n",
      "Epoch 606 Loss : 4861.752304136753 total_train_data : 7014\n",
      "Epoch 607 Loss : 4861.752339720726 total_train_data : 7014\n",
      "Epoch 608 Loss : 4861.75235080719 total_train_data : 7014\n",
      "Epoch 609 Loss : 4861.752322793007 total_train_data : 7014\n",
      "Epoch 610 Loss : 4861.752326607704 total_train_data : 7014\n",
      "Epoch 611 Loss : 4861.752319693565 total_train_data : 7014\n",
      "Epoch 612 Loss : 4861.752331614494 total_train_data : 7014\n",
      "Epoch 613 Loss : 4861.752357304096 total_train_data : 7014\n",
      "Epoch 614 Loss : 4861.752341866493 total_train_data : 7014\n",
      "Epoch 615 Loss : 4861.752341508865 total_train_data : 7014\n",
      "Epoch 616 Loss : 4861.752320587635 total_train_data : 7014\n",
      "Epoch 617 Loss : 4861.752314567566 total_train_data : 7014\n",
      "Epoch 618 Loss : 4861.752353847027 total_train_data : 7014\n",
      "Epoch 619 Loss : 4861.75234246254 total_train_data : 7014\n",
      "Epoch 620 Loss : 4861.752333343029 total_train_data : 7014\n",
      "Epoch 621 Loss : 4861.75231719017 total_train_data : 7014\n",
      "Epoch 622 Loss : 4861.752307355404 total_train_data : 7014\n",
      "Epoch 623 Loss : 4861.7523456811905 total_train_data : 7014\n",
      "Epoch 624 Loss : 4861.75235003233 total_train_data : 7014\n",
      "Epoch 625 Loss : 4861.75233322382 total_train_data : 7014\n",
      "Epoch 626 Loss : 4861.75232988596 total_train_data : 7014\n",
      "Epoch 627 Loss : 4861.752303540707 total_train_data : 7014\n",
      "Epoch 628 Loss : 4861.752307295799 total_train_data : 7014\n",
      "Epoch 629 Loss : 4861.752327501774 total_train_data : 7014\n",
      "Epoch 630 Loss : 4861.752353966236 total_train_data : 7014\n",
      "Epoch 631 Loss : 4861.75235003233 total_train_data : 7014\n",
      "Epoch 632 Loss : 4861.752341687679 total_train_data : 7014\n",
      "Epoch 633 Loss : 4861.752332508564 total_train_data : 7014\n",
      "Epoch 634 Loss : 4861.752325952053 total_train_data : 7014\n",
      "Epoch 635 Loss : 4861.752315104008 total_train_data : 7014\n",
      "Epoch 636 Loss : 4861.752308964729 total_train_data : 7014\n",
      "Epoch 637 Loss : 4861.752314627171 total_train_data : 7014\n",
      "Epoch 638 Loss : 4861.7523339390755 total_train_data : 7014\n",
      "Epoch 639 Loss : 4861.7523403167725 total_train_data : 7014\n",
      "Epoch 640 Loss : 4861.752340912819 total_train_data : 7014\n",
      "Epoch 641 Loss : 4861.7523502111435 total_train_data : 7014\n",
      "Epoch 642 Loss : 4861.752350509167 total_train_data : 7014\n",
      "Epoch 643 Loss : 4861.752343952656 total_train_data : 7014\n",
      "Epoch 644 Loss : 4861.752337634563 total_train_data : 7014\n",
      "Epoch 645 Loss : 4861.752320706844 total_train_data : 7014\n",
      "Epoch 646 Loss : 4861.7523137927055 total_train_data : 7014\n",
      "Epoch 647 Loss : 4861.752326667309 total_train_data : 7014\n",
      "Epoch 648 Loss : 4861.752318561077 total_train_data : 7014\n",
      "Epoch 649 Loss : 4861.752313673496 total_train_data : 7014\n",
      "Epoch 650 Loss : 4861.752325534821 total_train_data : 7014\n",
      "Epoch 651 Loss : 4861.752334654331 total_train_data : 7014\n",
      "Epoch 652 Loss : 4861.752325356007 total_train_data : 7014\n",
      "Epoch 653 Loss : 4861.752341270447 total_train_data : 7014\n",
      "Epoch 654 Loss : 4861.752345144749 total_train_data : 7014\n",
      "Epoch 655 Loss : 4861.7523403167725 total_train_data : 7014\n",
      "Epoch 656 Loss : 4861.752340912819 total_train_data : 7014\n",
      "Epoch 657 Loss : 4861.7523502111435 total_train_data : 7014\n",
      "Epoch 658 Loss : 4861.752350509167 total_train_data : 7014\n",
      "Epoch 659 Loss : 4861.752343952656 total_train_data : 7014\n",
      "Epoch 660 Loss : 4861.752337634563 total_train_data : 7014\n",
      "Epoch 661 Loss : 4861.75231897831 total_train_data : 7014\n",
      "Epoch 662 Loss : 4861.752313435078 total_train_data : 7014\n",
      "Epoch 663 Loss : 4861.752326667309 total_train_data : 7014\n",
      "Epoch 664 Loss : 4861.752318561077 total_train_data : 7014\n",
      "Epoch 665 Loss : 4861.752313673496 total_train_data : 7014\n",
      "Epoch 666 Loss : 4861.752325534821 total_train_data : 7014\n",
      "Epoch 667 Loss : 4861.752327024937 total_train_data : 7014\n",
      "Epoch 668 Loss : 4861.752325356007 total_train_data : 7014\n",
      "Epoch 669 Loss : 4861.752341270447 total_train_data : 7014\n",
      "Epoch 670 Loss : 4861.7523448467255 total_train_data : 7014\n",
      "Epoch 671 Loss : 4861.75232988596 total_train_data : 7014\n",
      "Epoch 672 Loss : 4861.752340912819 total_train_data : 7014\n",
      "Epoch 673 Loss : 4861.752350866795 total_train_data : 7014\n",
      "Epoch 674 Loss : 4861.752352297306 total_train_data : 7014\n",
      "Epoch 675 Loss : 4861.752345561981 total_train_data : 7014\n",
      "Epoch 676 Loss : 4861.752338826656 total_train_data : 7014\n",
      "Epoch 677 Loss : 4861.752320766449 total_train_data : 7014\n",
      "Epoch 678 Loss : 4861.752315223217 total_train_data : 7014\n",
      "Epoch 679 Loss : 4861.752328455448 total_train_data : 7014\n",
      "Epoch 680 Loss : 4861.752321004868 total_train_data : 7014\n",
      "Epoch 681 Loss : 4861.752316594124 total_train_data : 7014\n",
      "Epoch 682 Loss : 4861.75232732296 total_train_data : 7014\n",
      "Epoch 683 Loss : 4861.752328634262 total_train_data : 7014\n",
      "Epoch 684 Loss : 4861.752325892448 total_train_data : 7014\n",
      "Epoch 685 Loss : 4861.752341926098 total_train_data : 7014\n",
      "Epoch 686 Loss : 4861.752346634865 total_train_data : 7014\n",
      "Epoch 687 Loss : 4861.752331674099 total_train_data : 7014\n",
      "Epoch 688 Loss : 4861.752342045307 total_train_data : 7014\n",
      "Epoch 689 Loss : 4861.752350866795 total_train_data : 7014\n",
      "Epoch 690 Loss : 4861.752352297306 total_train_data : 7014\n",
      "Epoch 691 Loss : 4861.752345561981 total_train_data : 7014\n",
      "Epoch 692 Loss : 4861.752338826656 total_train_data : 7014\n",
      "Epoch 693 Loss : 4861.752320766449 total_train_data : 7014\n",
      "Epoch 694 Loss : 4861.752315223217 total_train_data : 7014\n",
      "Epoch 695 Loss : 4861.752328455448 total_train_data : 7014\n",
      "Epoch 696 Loss : 4861.752321004868 total_train_data : 7014\n",
      "Epoch 697 Loss : 4861.75231641531 total_train_data : 7014\n",
      "Epoch 698 Loss : 4861.752327084541 total_train_data : 7014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 699 Loss : 4861.752328634262 total_train_data : 7014\n",
      "Epoch 700 Loss : 4861.752325892448 total_train_data : 7014\n",
      "Epoch 701 Loss : 4861.752341926098 total_train_data : 7014\n",
      "Epoch 702 Loss : 4861.752346634865 total_train_data : 7014\n",
      "Epoch 703 Loss : 4861.752331674099 total_train_data : 7014\n",
      "Epoch 704 Loss : 4861.752342045307 total_train_data : 7014\n",
      "Epoch 705 Loss : 4861.752350866795 total_train_data : 7014\n",
      "Epoch 706 Loss : 4861.752351522446 total_train_data : 7014\n",
      "Epoch 707 Loss : 4861.752342641354 total_train_data : 7014\n",
      "Epoch 708 Loss : 4861.752338826656 total_train_data : 7014\n",
      "Epoch 709 Loss : 4861.752320587635 total_train_data : 7014\n",
      "Epoch 710 Loss : 4861.752314984798 total_train_data : 7014\n",
      "Epoch 711 Loss : 4861.752325594425 total_train_data : 7014\n",
      "Epoch 712 Loss : 4861.752321004868 total_train_data : 7014\n",
      "Epoch 713 Loss : 4861.75231641531 total_train_data : 7014\n",
      "Epoch 714 Loss : 4861.752327084541 total_train_data : 7014\n",
      "Epoch 715 Loss : 4861.752328634262 total_train_data : 7014\n",
      "Epoch 716 Loss : 4861.752325892448 total_train_data : 7014\n",
      "Epoch 717 Loss : 4861.752341926098 total_train_data : 7014\n",
      "Epoch 718 Loss : 4861.752345860004 total_train_data : 7014\n",
      "Epoch 719 Loss : 4861.752331614494 total_train_data : 7014\n",
      "Epoch 720 Loss : 4861.752342045307 total_train_data : 7014\n",
      "Epoch 721 Loss : 4861.752350866795 total_train_data : 7014\n",
      "Epoch 722 Loss : 4861.752351522446 total_train_data : 7014\n",
      "Epoch 723 Loss : 4861.752332448959 total_train_data : 7014\n",
      "Epoch 724 Loss : 4861.752338826656 total_train_data : 7014\n",
      "Epoch 725 Loss : 4861.752320587635 total_train_data : 7014\n",
      "Epoch 726 Loss : 4861.752314984798 total_train_data : 7014\n",
      "Epoch 727 Loss : 4861.7523188591 total_train_data : 7014\n",
      "Epoch 728 Loss : 4861.752321004868 total_train_data : 7014\n",
      "Epoch 729 Loss : 4861.75231641531 total_train_data : 7014\n",
      "Epoch 730 Loss : 4861.752327084541 total_train_data : 7014\n",
      "Epoch 731 Loss : 4861.752328634262 total_train_data : 7014\n",
      "Epoch 732 Loss : 4861.752325892448 total_train_data : 7014\n",
      "Epoch 733 Loss : 4861.752341926098 total_train_data : 7014\n",
      "Epoch 734 Loss : 4861.752345860004 total_train_data : 7014\n",
      "Epoch 735 Loss : 4861.752331614494 total_train_data : 7014\n",
      "Epoch 736 Loss : 4861.752342045307 total_train_data : 7014\n",
      "Epoch 737 Loss : 4861.752350747585 total_train_data : 7014\n",
      "Epoch 738 Loss : 4861.752347826958 total_train_data : 7014\n",
      "Epoch 739 Loss : 4861.752332448959 total_train_data : 7014\n",
      "Epoch 740 Loss : 4861.7523357868195 total_train_data : 7014\n",
      "Epoch 741 Loss : 4861.752320587635 total_train_data : 7014\n",
      "Epoch 742 Loss : 4861.752314984798 total_train_data : 7014\n",
      "Epoch 743 Loss : 4861.7523188591 total_train_data : 7014\n",
      "Epoch 744 Loss : 4861.752317965031 total_train_data : 7014\n",
      "Epoch 745 Loss : 4861.75231641531 total_train_data : 7014\n",
      "Epoch 746 Loss : 4861.752327084541 total_train_data : 7014\n",
      "Epoch 747 Loss : 4861.752328634262 total_train_data : 7014\n",
      "Epoch 748 Loss : 4861.752325773239 total_train_data : 7014\n",
      "Epoch 749 Loss : 4861.75233912468 total_train_data : 7014\n",
      "Epoch 750 Loss : 4861.7523406744 total_train_data : 7014\n",
      "Epoch 751 Loss : 4861.752331614494 total_train_data : 7014\n",
      "Epoch 752 Loss : 4861.752342045307 total_train_data : 7014\n",
      "Epoch 753 Loss : 4861.752350747585 total_train_data : 7014\n",
      "Epoch 754 Loss : 4861.752347826958 total_train_data : 7014\n",
      "Epoch 755 Loss : 4861.752332448959 total_train_data : 7014\n",
      "Epoch 756 Loss : 4861.7523357868195 total_train_data : 7014\n",
      "Epoch 757 Loss : 4861.752320587635 total_train_data : 7014\n",
      "Epoch 758 Loss : 4861.752314984798 total_train_data : 7014\n",
      "Epoch 759 Loss : 4861.7523188591 total_train_data : 7014\n",
      "Epoch 760 Loss : 4861.752317845821 total_train_data : 7014\n",
      "Epoch 761 Loss : 4861.75231385231 total_train_data : 7014\n",
      "Epoch 762 Loss : 4861.752327084541 total_train_data : 7014\n",
      "Epoch 763 Loss : 4861.752328634262 total_train_data : 7014\n",
      "Epoch 764 Loss : 4861.752325773239 total_train_data : 7014\n",
      "Epoch 765 Loss : 4861.75233912468 total_train_data : 7014\n",
      "Epoch 766 Loss : 4861.7523406744 total_train_data : 7014\n",
      "Epoch 767 Loss : 4861.752331614494 total_train_data : 7014\n",
      "Epoch 768 Loss : 4861.752342045307 total_train_data : 7014\n",
      "Epoch 769 Loss : 4861.752350747585 total_train_data : 7014\n",
      "Epoch 770 Loss : 4861.752347826958 total_train_data : 7014\n",
      "Epoch 771 Loss : 4861.752332448959 total_train_data : 7014\n",
      "Epoch 772 Loss : 4861.7523357868195 total_train_data : 7014\n",
      "Epoch 773 Loss : 4861.752320587635 total_train_data : 7014\n",
      "Epoch 774 Loss : 4861.752314984798 total_train_data : 7014\n",
      "Epoch 775 Loss : 4861.7523188591 total_train_data : 7014\n",
      "Epoch 776 Loss : 4861.752317845821 total_train_data : 7014\n",
      "Epoch 777 Loss : 4861.75231385231 total_train_data : 7014\n",
      "Epoch 778 Loss : 4861.752327084541 total_train_data : 7014\n",
      "Epoch 779 Loss : 4861.752328634262 total_train_data : 7014\n",
      "Epoch 780 Loss : 4861.752325773239 total_train_data : 7014\n",
      "Epoch 781 Loss : 4861.75233912468 total_train_data : 7014\n",
      "Epoch 782 Loss : 4861.7523406744 total_train_data : 7014\n",
      "Epoch 783 Loss : 4861.752331614494 total_train_data : 7014\n",
      "Epoch 784 Loss : 4861.752342045307 total_train_data : 7014\n",
      "Epoch 785 Loss : 4861.752350747585 total_train_data : 7014\n",
      "Epoch 786 Loss : 4861.752347826958 total_train_data : 7014\n",
      "Epoch 787 Loss : 4861.752332448959 total_train_data : 7014\n",
      "Epoch 788 Loss : 4861.7523357868195 total_train_data : 7014\n",
      "Epoch 789 Loss : 4861.752320587635 total_train_data : 7014\n",
      "Epoch 790 Loss : 4861.7523139715195 total_train_data : 7014\n",
      "Epoch 791 Loss : 4861.75231474638 total_train_data : 7014\n",
      "Epoch 792 Loss : 4861.752317845821 total_train_data : 7014\n",
      "Epoch 793 Loss : 4861.75231385231 total_train_data : 7014\n",
      "Epoch 794 Loss : 4861.752327084541 total_train_data : 7014\n",
      "Epoch 795 Loss : 4861.752328634262 total_train_data : 7014\n",
      "Epoch 796 Loss : 4861.752325773239 total_train_data : 7014\n",
      "Epoch 797 Loss : 4861.75233912468 total_train_data : 7014\n",
      "Epoch 798 Loss : 4861.7523406744 total_train_data : 7014\n",
      "Epoch 799 Loss : 4861.752331614494 total_train_data : 7014\n",
      "Epoch 800 Loss : 4861.752342045307 total_train_data : 7014\n",
      "Epoch 801 Loss : 4861.752350747585 total_train_data : 7014\n",
      "Epoch 802 Loss : 4861.752346813679 total_train_data : 7014\n",
      "Epoch 803 Loss : 4861.752328336239 total_train_data : 7014\n",
      "Epoch 804 Loss : 4861.7523357868195 total_train_data : 7014\n",
      "Epoch 805 Loss : 4861.752320587635 total_train_data : 7014\n",
      "Epoch 806 Loss : 4861.7523139715195 total_train_data : 7014\n",
      "Epoch 807 Loss : 4861.75231474638 total_train_data : 7014\n",
      "Epoch 808 Loss : 4861.752317845821 total_train_data : 7014\n",
      "Epoch 809 Loss : 4861.75231385231 total_train_data : 7014\n",
      "Epoch 810 Loss : 4861.752324581146 total_train_data : 7014\n",
      "Epoch 811 Loss : 4861.752328634262 total_train_data : 7014\n",
      "Epoch 812 Loss : 4861.752325773239 total_train_data : 7014\n",
      "Epoch 813 Loss : 4861.75233912468 total_train_data : 7014\n",
      "Epoch 814 Loss : 4861.7523406744 total_train_data : 7014\n",
      "Epoch 815 Loss : 4861.752331614494 total_train_data : 7014\n",
      "Epoch 816 Loss : 4861.752342045307 total_train_data : 7014\n",
      "Epoch 817 Loss : 4861.752350747585 total_train_data : 7014\n",
      "Epoch 818 Loss : 4861.752346813679 total_train_data : 7014\n",
      "Epoch 819 Loss : 4861.752327799797 total_train_data : 7014\n",
      "Epoch 820 Loss : 4861.752334415913 total_train_data : 7014\n",
      "Epoch 821 Loss : 4861.752320587635 total_train_data : 7014\n",
      "Epoch 822 Loss : 4861.752312839031 total_train_data : 7014\n",
      "Epoch 823 Loss : 4861.752314388752 total_train_data : 7014\n",
      "Epoch 824 Loss : 4861.75231730938 total_train_data : 7014\n",
      "Epoch 825 Loss : 4861.75231385231 total_train_data : 7014\n",
      "Epoch 826 Loss : 4861.752324581146 total_train_data : 7014\n",
      "Epoch 827 Loss : 4861.752328634262 total_train_data : 7014\n",
      "Epoch 828 Loss : 4861.752325773239 total_train_data : 7014\n",
      "Epoch 829 Loss : 4861.75233912468 total_train_data : 7014\n",
      "Epoch 830 Loss : 4861.7523406744 total_train_data : 7014\n",
      "Epoch 831 Loss : 4861.752331614494 total_train_data : 7014\n",
      "Epoch 832 Loss : 4861.752342045307 total_train_data : 7014\n",
      "Epoch 833 Loss : 4861.752346992493 total_train_data : 7014\n",
      "Epoch 834 Loss : 4861.752346813679 total_train_data : 7014\n",
      "Epoch 835 Loss : 4861.752327799797 total_train_data : 7014\n",
      "Epoch 836 Loss : 4861.752334415913 total_train_data : 7014\n",
      "Epoch 837 Loss : 4861.752320587635 total_train_data : 7014\n",
      "Epoch 838 Loss : 4861.752312839031 total_train_data : 7014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 839 Loss : 4861.75231218338 total_train_data : 7014\n",
      "Epoch 840 Loss : 4861.75231730938 total_train_data : 7014\n",
      "Epoch 841 Loss : 4861.75231385231 total_train_data : 7014\n",
      "Epoch 842 Loss : 4861.752324581146 total_train_data : 7014\n",
      "Epoch 843 Loss : 4861.752328634262 total_train_data : 7014\n",
      "Epoch 844 Loss : 4861.752325773239 total_train_data : 7014\n",
      "Epoch 845 Loss : 4861.752331614494 total_train_data : 7014\n",
      "Epoch 846 Loss : 4861.752336144447 total_train_data : 7014\n",
      "Epoch 847 Loss : 4861.752331614494 total_train_data : 7014\n",
      "Epoch 848 Loss : 4861.752342045307 total_train_data : 7014\n",
      "Epoch 849 Loss : 4861.752346992493 total_train_data : 7014\n",
      "Epoch 850 Loss : 4861.752346813679 total_train_data : 7014\n",
      "Epoch 851 Loss : 4861.752325594425 total_train_data : 7014\n",
      "Epoch 852 Loss : 4861.752334415913 total_train_data : 7014\n",
      "Epoch 853 Loss : 4861.752320587635 total_train_data : 7014\n",
      "Epoch 854 Loss : 4861.752312839031 total_train_data : 7014\n",
      "Epoch 855 Loss : 4861.75231218338 total_train_data : 7014\n",
      "Epoch 856 Loss : 4861.752306103706 total_train_data : 7014\n",
      "Epoch 857 Loss : 4861.75231385231 total_train_data : 7014\n",
      "Epoch 858 Loss : 4861.75232231617 total_train_data : 7014\n",
      "Epoch 859 Loss : 4861.7523283958435 total_train_data : 7014\n",
      "Epoch 860 Loss : 4861.752325236797 total_train_data : 7014\n",
      "Epoch 861 Loss : 4861.752331614494 total_train_data : 7014\n",
      "Epoch 862 Loss : 4861.752336144447 total_train_data : 7014\n",
      "Epoch 863 Loss : 4861.752333879471 total_train_data : 7014\n",
      "Epoch 864 Loss : 4861.752344310284 total_train_data : 7014\n",
      "Epoch 865 Loss : 4861.75234824419 total_train_data : 7014\n",
      "Epoch 866 Loss : 4861.752349078655 total_train_data : 7014\n",
      "Epoch 867 Loss : 4861.752327859402 total_train_data : 7014\n",
      "Epoch 868 Loss : 4861.752325475216 total_train_data : 7014\n",
      "Epoch 869 Loss : 4861.752314686775 total_train_data : 7014\n",
      "Epoch 870 Loss : 4861.752317368984 total_train_data : 7014\n",
      "Epoch 871 Loss : 4861.752314388752 total_train_data : 7014\n",
      "Epoch 872 Loss : 4861.752308130264 total_train_data : 7014\n",
      "Epoch 873 Loss : 4861.752315104008 total_train_data : 7014\n",
      "Epoch 874 Loss : 4861.752324581146 total_train_data : 7014\n",
      "Epoch 875 Loss : 4861.752321302891 total_train_data : 7014\n",
      "Epoch 876 Loss : 4861.752327203751 total_train_data : 7014\n",
      "Epoch 877 Loss : 4861.752333879471 total_train_data : 7014\n",
      "Epoch 878 Loss : 4861.7523406744 total_train_data : 7014\n",
      "Epoch 879 Loss : 4861.752333879471 total_train_data : 7014\n",
      "Epoch 880 Loss : 4861.752344310284 total_train_data : 7014\n",
      "Epoch 881 Loss : 4861.75234824419 total_train_data : 7014\n",
      "Epoch 882 Loss : 4861.752349078655 total_train_data : 7014\n",
      "Epoch 883 Loss : 4861.752327859402 total_train_data : 7014\n",
      "Epoch 884 Loss : 4861.752325475216 total_train_data : 7014\n",
      "Epoch 885 Loss : 4861.752314686775 total_train_data : 7014\n",
      "Epoch 886 Loss : 4861.752316296101 total_train_data : 7014\n",
      "Epoch 887 Loss : 4861.752305030823 total_train_data : 7014\n",
      "Epoch 888 Loss : 4861.752307832241 total_train_data : 7014\n",
      "Epoch 889 Loss : 4861.752315104008 total_train_data : 7014\n",
      "Epoch 890 Loss : 4861.752324581146 total_train_data : 7014\n",
      "Epoch 891 Loss : 4861.752321302891 total_train_data : 7014\n",
      "Epoch 892 Loss : 4861.752327203751 total_train_data : 7014\n",
      "Epoch 893 Loss : 4861.752333879471 total_train_data : 7014\n",
      "Epoch 894 Loss : 4861.7523406744 total_train_data : 7014\n",
      "Epoch 895 Loss : 4861.752333879471 total_train_data : 7014\n",
      "Epoch 896 Loss : 4861.752344310284 total_train_data : 7014\n",
      "Epoch 897 Loss : 4861.75234824419 total_train_data : 7014\n",
      "Epoch 898 Loss : 4861.752348005772 total_train_data : 7014\n",
      "Epoch 899 Loss : 4861.752327799797 total_train_data : 7014\n",
      "Epoch 900 Loss : 4861.752314925194 total_train_data : 7014\n",
      "Epoch 901 Loss : 4861.752314269543 total_train_data : 7014\n",
      "Epoch 902 Loss : 4861.752316296101 total_train_data : 7014\n",
      "Epoch 903 Loss : 4861.752305030823 total_train_data : 7014\n",
      "Epoch 904 Loss : 4861.752306640148 total_train_data : 7014\n",
      "Epoch 905 Loss : 4861.752315104008 total_train_data : 7014\n",
      "Epoch 906 Loss : 4861.752324581146 total_train_data : 7014\n",
      "Epoch 907 Loss : 4861.752321064472 total_train_data : 7014\n",
      "Epoch 908 Loss : 4861.7523247003555 total_train_data : 7014\n",
      "Epoch 909 Loss : 4861.752333879471 total_train_data : 7014\n",
      "Epoch 910 Loss : 4861.7523406744 total_train_data : 7014\n",
      "Epoch 911 Loss : 4861.752333819866 total_train_data : 7014\n",
      "Epoch 912 Loss : 4861.752335906029 total_train_data : 7014\n",
      "Epoch 913 Loss : 4861.752348005772 total_train_data : 7014\n",
      "Epoch 914 Loss : 4861.752337634563 total_train_data : 7014\n",
      "Epoch 915 Loss : 4861.752327799797 total_train_data : 7014\n",
      "Epoch 916 Loss : 4861.752314925194 total_train_data : 7014\n",
      "Epoch 917 Loss : 4861.752314269543 total_train_data : 7014\n",
      "Epoch 918 Loss : 4861.752316296101 total_train_data : 7014\n",
      "Epoch 919 Loss : 4861.752304553986 total_train_data : 7014\n",
      "Epoch 920 Loss : 4861.752301752567 total_train_data : 7014\n",
      "Epoch 921 Loss : 4861.752315104008 total_train_data : 7014\n",
      "Epoch 922 Loss : 4861.752324581146 total_train_data : 7014\n",
      "Epoch 923 Loss : 4861.752321064472 total_train_data : 7014\n",
      "Epoch 924 Loss : 4861.7523247003555 total_train_data : 7014\n",
      "Epoch 925 Loss : 4861.752333879471 total_train_data : 7014\n",
      "Epoch 926 Loss : 4861.752333462238 total_train_data : 7014\n",
      "Epoch 927 Loss : 4861.752333819866 total_train_data : 7014\n",
      "Epoch 928 Loss : 4861.752335906029 total_train_data : 7014\n",
      "Epoch 929 Loss : 4861.752348005772 total_train_data : 7014\n",
      "Epoch 930 Loss : 4861.75233733654 total_train_data : 7014\n",
      "Epoch 931 Loss : 4861.752326250076 total_train_data : 7014\n",
      "Epoch 932 Loss : 4861.752314925194 total_train_data : 7014\n",
      "Epoch 933 Loss : 4861.752314269543 total_train_data : 7014\n",
      "Epoch 934 Loss : 4861.752316296101 total_train_data : 7014\n",
      "Epoch 935 Loss : 4861.752304553986 total_train_data : 7014\n",
      "Epoch 936 Loss : 4861.752301752567 total_train_data : 7014\n",
      "Epoch 937 Loss : 4861.752315104008 total_train_data : 7014\n",
      "Epoch 938 Loss : 4861.752324581146 total_train_data : 7014\n",
      "Epoch 939 Loss : 4861.752321064472 total_train_data : 7014\n",
      "Epoch 940 Loss : 4861.7523247003555 total_train_data : 7014\n",
      "Epoch 941 Loss : 4861.752333879471 total_train_data : 7014\n",
      "Epoch 942 Loss : 4861.752324402332 total_train_data : 7014\n",
      "Epoch 943 Loss : 4861.752332270145 total_train_data : 7014\n",
      "Epoch 944 Loss : 4861.752335906029 total_train_data : 7014\n",
      "Epoch 945 Loss : 4861.752348005772 total_train_data : 7014\n",
      "Epoch 946 Loss : 4861.75233733654 total_train_data : 7014\n",
      "Epoch 947 Loss : 4861.752326250076 total_train_data : 7014\n",
      "Epoch 948 Loss : 4861.752314925194 total_train_data : 7014\n",
      "Epoch 949 Loss : 4861.752314269543 total_train_data : 7014\n",
      "Epoch 950 Loss : 4861.752316296101 total_train_data : 7014\n",
      "Epoch 951 Loss : 4861.752304553986 total_train_data : 7014\n",
      "Epoch 952 Loss : 4861.752301752567 total_train_data : 7014\n",
      "Epoch 953 Loss : 4861.752315104008 total_train_data : 7014\n",
      "Epoch 954 Loss : 4861.752318024635 total_train_data : 7014\n",
      "Epoch 955 Loss : 4861.752321064472 total_train_data : 7014\n",
      "Epoch 956 Loss : 4861.7523247003555 total_train_data : 7014\n",
      "Epoch 957 Loss : 4861.752333879471 total_train_data : 7014\n",
      "Epoch 958 Loss : 4861.752324402332 total_train_data : 7014\n",
      "Epoch 959 Loss : 4861.752332270145 total_train_data : 7014\n",
      "Epoch 960 Loss : 4861.752335906029 total_train_data : 7014\n",
      "Epoch 961 Loss : 4861.752348005772 total_train_data : 7014\n",
      "Epoch 962 Loss : 4861.75233733654 total_train_data : 7014\n",
      "Epoch 963 Loss : 4861.752326250076 total_train_data : 7014\n",
      "Epoch 964 Loss : 4861.752314925194 total_train_data : 7014\n",
      "Epoch 965 Loss : 4861.752314269543 total_train_data : 7014\n",
      "Epoch 966 Loss : 4861.752316296101 total_train_data : 7014\n",
      "Epoch 967 Loss : 4861.752304553986 total_train_data : 7014\n",
      "Epoch 968 Loss : 4861.752301752567 total_train_data : 7014\n",
      "Epoch 969 Loss : 4861.752315104008 total_train_data : 7014\n",
      "Epoch 970 Loss : 4861.752318024635 total_train_data : 7014\n",
      "Epoch 971 Loss : 4861.752321064472 total_train_data : 7014\n",
      "Epoch 972 Loss : 4861.7523247003555 total_train_data : 7014\n",
      "Epoch 973 Loss : 4861.752333879471 total_train_data : 7014\n",
      "Epoch 974 Loss : 4861.752324402332 total_train_data : 7014\n",
      "Epoch 975 Loss : 4861.752332270145 total_train_data : 7014\n",
      "Epoch 976 Loss : 4861.752335906029 total_train_data : 7014\n",
      "Epoch 977 Loss : 4861.752348005772 total_train_data : 7014\n",
      "Epoch 978 Loss : 4861.75233733654 total_train_data : 7014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 979 Loss : 4861.752326250076 total_train_data : 7014\n",
      "Epoch 980 Loss : 4861.752314925194 total_train_data : 7014\n",
      "Epoch 981 Loss : 4861.752314269543 total_train_data : 7014\n",
      "Epoch 982 Loss : 4861.752316296101 total_train_data : 7014\n",
      "Epoch 983 Loss : 4861.752304553986 total_train_data : 7014\n",
      "Epoch 984 Loss : 4861.752301752567 total_train_data : 7014\n",
      "Epoch 985 Loss : 4861.752315104008 total_train_data : 7014\n",
      "Epoch 986 Loss : 4861.752318024635 total_train_data : 7014\n",
      "Epoch 987 Loss : 4861.752321064472 total_train_data : 7014\n",
      "Epoch 988 Loss : 4861.7523247003555 total_train_data : 7014\n",
      "Epoch 989 Loss : 4861.752333879471 total_train_data : 7014\n",
      "Epoch 990 Loss : 4861.752324402332 total_train_data : 7014\n",
      "Epoch 991 Loss : 4861.752324581146 total_train_data : 7014\n",
      "Epoch 992 Loss : 4861.752335906029 total_train_data : 7014\n",
      "Epoch 993 Loss : 4861.752348005772 total_train_data : 7014\n",
      "Epoch 994 Loss : 4861.75233733654 total_train_data : 7014\n",
      "Epoch 995 Loss : 4861.752326250076 total_train_data : 7014\n",
      "Epoch 996 Loss : 4861.752314925194 total_train_data : 7014\n",
      "Epoch 997 Loss : 4861.752314269543 total_train_data : 7014\n",
      "Epoch 998 Loss : 4861.752316296101 total_train_data : 7014\n",
      "Epoch 999 Loss : 4861.752304553986 total_train_data : 7014\n"
     ]
    }
   ],
   "source": [
    "train_v2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict():\n",
    "    model_v2 = Convolution_1D_Model(total_tag= input_size_v2, \n",
    "                                    embeding_size= embedding_size_v2, \n",
    "                                    filter_num_1= filter_num_1_v2, \n",
    "                                    filter_size_1= filter_size_1_v2, \n",
    "                                    filter_num_2= filter_num_2_v2, \n",
    "                                    filter_size_2=filter_size_2_v2 ,\n",
    "                                    output_size = output_size_v2, \n",
    "                                    max_word= max_word_v2 )\n",
    "    model_v2 = torch.load( '../pickle/model_v2.pt')\n",
    "    total_num = 0\n",
    "    correct = 0\n",
    "    for seq_tag_idx in range(len(training_data_tag_list)):\n",
    "        \n",
    "        if len(training_data_tag_list[seq_tag_idx]) > max_word_v2:\n",
    "            continue\n",
    "        total_num += 1\n",
    "        input_data = torch.LongTensor([dict_tag2idx[t] for t in training_data_tag_list[seq_tag_idx]])\n",
    "        target_data = torch.LongTensor([training_target_sentence_list[seq_tag_idx]])\n",
    "        out = model_v2(input_data)\n",
    "        \n",
    "        if out[0].item() > out[1].item() and target_data[0].item() ==0:\n",
    "            correct += 1\n",
    "        elif out[0].item() < out[1].item() and target_data[0].item() ==1:\n",
    "            correct += 1\n",
    "        else:\n",
    "            continue\n",
    "    print (correct / total_num )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:67: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
