{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the using package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import requests\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import monpa\n",
    "import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Architecture of the Designed Model using only Bi-GRU \n",
    "<img src=\"https://i.imgur.com/zjsZb0l.png\" width = \"300\" height = \"200\" alt=\"design_model\" align=center />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolution_1D_Model (nn.Module):\n",
    "    def __init__(self, total_tag, embeding_size, filter_num_1, filter_size_1, filter_num_2, filter_size_2 , output_size,max_word):\n",
    "        super(Convolution_1D_Model, self).__init__()\n",
    "        self.total_tag = total_tag\n",
    "        self.max_word = max_word\n",
    "        self.embeding_size = embeding_size\n",
    "        self.filter_num_1 = filter_num_1\n",
    "        self.filter_size_1 = filter_size_1\n",
    "        self.filter_num_2 = filter_num_2\n",
    "        self.filter_size_2 = filter_size_2\n",
    "        self.output_size = output_size\n",
    "        self.Embeding = nn.Embedding(self.total_tag, self.embeding_size)\n",
    "        self.Conv1d_layer1 = nn.Sequential(nn.Conv1d(in_channels=self.embeding_size, \n",
    "                                                            out_channels=self.filter_num_1,\n",
    "                                                            kernel_size=self.filter_size_1),\n",
    "                                                         #nn.BatchNorm1d (self.filter_size_1),\n",
    "                                                         nn.ReLU())\n",
    "        \n",
    "        self.Conv1d_layer2 = nn.Sequential(nn.Conv1d(in_channels=self.filter_num_1, \n",
    "                                                            out_channels=self.filter_num_2,\n",
    "                                                            kernel_size=self.filter_size_2),\n",
    "                                                         #nn.BatchNorm1d (self.filter_size_2),\n",
    "                                                         nn.ReLU())\n",
    "        self.Conv1d_layer3 = nn.Sequential(nn.Conv1d(in_channels=self.filter_num_1, \n",
    "                                                            out_channels=self.filter_num_2,\n",
    "                                                            kernel_size=self.filter_size_2),\n",
    "                                                         #nn.BatchNorm1d (self.filter_size_2),\n",
    "                                                         nn.ReLU())\n",
    "        self.Conv1d_layer4 = nn.Sequential(nn.Conv1d(in_channels=self.filter_num_1, \n",
    "                                                            out_channels=self.filter_num_2,\n",
    "                                                            kernel_size=self.filter_size_2),\n",
    "                                                         #nn.BatchNorm1d (self.filter_size_2),\n",
    "                                                         nn.ReLU())\n",
    "        # batch size is  set to 1 , output_size is set to 2\n",
    "        L = (self.max_word- self.filter_size_1+ 1)- self.filter_size_2+ 1\n",
    "        self.linear = nn.Linear( self.filter_num_2 * L, self.output_size )\n",
    "        \n",
    "    def forward(self, input_data):  \n",
    "        embed_res = self.Embeding (input_data)\n",
    "        if len(input_data) < self.max_word:\n",
    "            for c in range (self.max_word - len(input_data)):\n",
    "                embed_res = torch.cat((embed_res, torch.zeros(1, self.embeding_size)),dim = 0 )\n",
    "        embed_res = embed_res.unsqueeze(0)\n",
    "        #print (' Embedding :::: ' , embed_res.size())\n",
    "        # let the input dimension be the (N , Cin , L) and then the output with the dimension (N, Cout , Lout)\n",
    "        conv_res1 = self.Conv1d_layer1 (embed_res.permute(0,2,1))\n",
    "        #print (' conv_res1 :::: ' , conv_res1.size())\n",
    "        conv_res2 = self.Conv1d_layer2 (conv_res1)\n",
    "        #print ('conv_res2 :::: ' , conv_res2.size())\n",
    "        linear_res = self.linear(conv_res2.view(-1))\n",
    "        final_res = F.log_softmax(linear_res)\n",
    "        #print (\"final_res :::: \" , final_res)\n",
    "        return final_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V2_Model_with_only_convolution1D ::: \n",
      " Convolution_1D_Model(\n",
      "  (Embeding): Embedding(16, 8)\n",
      "  (Conv1d_layer1): Sequential(\n",
      "    (0): Conv1d(8, 5, kernel_size=(2,), stride=(1,))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (Conv1d_layer2): Sequential(\n",
      "    (0): Conv1d(5, 5, kernel_size=(2,), stride=(1,))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (linear): Linear(in_features=40, out_features=7, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:41: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "model_v2 = Convolution_1D_Model(total_tag=16, embeding_size=8, filter_num_1=5, filter_size_1=2, filter_num_2=5, filter_size_2=2 ,output_size = 7, max_word=10)\n",
    "test_vec = torch.LongTensor([1,2,3,4,5,6,7])\n",
    "model_v2(test_vec)\n",
    "print ('V2_Model_with_only_convolution1D ::: \\n', model_v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing\n",
    "\n",
    "* 將positive_data和negative_data載入存入List\n",
    "* 將monpa的tags載入dictionary\n",
    "* 將sentence 的 words 轉成詞性\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"training_data_sentence_list = list()\\ntraining_data_tag_list = list()\\ntraining_target_sentence_list = list()\\ndict_idx2tag = dict()\\ndict_tag2idx = dict()\\n\\n# Read both the postive data and negative data\\nwith open ('../CJX_Train_Test_Data/positive_data.txt', 'r', encoding = 'utf-8') as rf:\\n    for pos_s in rf.readlines():\\n        training_data_sentence_list.append(pos_s.strip('\\n').strip(',').strip('，').strip('《').strip('》').strip('【').strip('】').strip('、').strip('、').strip('-'))\\n        training_target_sentence_list.append(1)\\nwith open ('../CJX_Train_Test_Data/positive_data.txt', 'r', encoding = 'utf-8') as rf:\\n    for pos_s in rf.readlines():\\n        training_data_sentence_list.append(pos_s.strip('\\n').strip(',').strip('，').strip('《').strip('》').strip('【').strip('】').strip('、').strip('-'))\\n        training_target_sentence_list.append(0)\\n        \\nprint ('training data size : ',len(training_data_sentence_list))\\nprint ('training data target size : ',len(training_target_sentence_list))\\ntmp_cnt = 0\\n# Change the words to tags, because we need to use the sequential tags for training.\\nfor sentence in training_data_sentence_list:\\n    tmp_cnt += 1\\n    #print (tmp_cnt)\\n    tmp = monpa.pseg(sentence)\\n    tmp_list = list()\\n    for item in tmp:\\n        tmp_list.append(item[1])\\n    training_data_tag_list.append(tmp_list)\\n    \\nprint (len(training_data_tag_list))\\n\\n# Read the tags of the monpa\\nwith open ('./monpa_tag.txt' , 'r' , encoding = 'utf-8') as rf:\\n    cnt = 0\\n    for tag in rf.readlines():\\n        dict_idx2tag[cnt] = tag.strip('\\n')\\n        dict_tag2idx[tag.strip('\\n')] = cnt\\n        cnt += 1  \\n\\nprint ('The dictionary of idxs 2 tags : ', dict_idx2tag)\\nprint ('The dictionary of tags 2 idxs : ' , dict_tag2idx)\\n\\nshuffle_zip = list(zip(training_data_tag_list, training_target_sentence_list))\\nrandom.shuffle(shuffle_zip)\\ntraining_data_tag_list[:], training_target_sentence_list[:] = zip(*shuffle_zip)\\nprint (training_data_tag_list, training_target_sentence_list)\\n\""
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''training_data_sentence_list = list()\n",
    "training_data_tag_list = list()\n",
    "training_target_sentence_list = list()\n",
    "dict_idx2tag = dict()\n",
    "dict_tag2idx = dict()\n",
    "\n",
    "# Read both the postive data and negative data\n",
    "with open ('../CJX_Train_Test_Data/positive_data.txt', 'r', encoding = 'utf-8') as rf:\n",
    "    for pos_s in rf.readlines():\n",
    "        training_data_sentence_list.append(pos_s.strip('\\n').strip(',').strip('，').strip('《').strip('》').strip('【').strip('】').strip('、').strip('、').strip('-'))\n",
    "        training_target_sentence_list.append(1)\n",
    "with open ('../CJX_Train_Test_Data/positive_data.txt', 'r', encoding = 'utf-8') as rf:\n",
    "    for pos_s in rf.readlines():\n",
    "        training_data_sentence_list.append(pos_s.strip('\\n').strip(',').strip('，').strip('《').strip('》').strip('【').strip('】').strip('、').strip('-'))\n",
    "        training_target_sentence_list.append(0)\n",
    "        \n",
    "print ('training data size : ',len(training_data_sentence_list))\n",
    "print ('training data target size : ',len(training_target_sentence_list))\n",
    "tmp_cnt = 0\n",
    "# Change the words to tags, because we need to use the sequential tags for training.\n",
    "for sentence in training_data_sentence_list:\n",
    "    tmp_cnt += 1\n",
    "    #print (tmp_cnt)\n",
    "    tmp = monpa.pseg(sentence)\n",
    "    tmp_list = list()\n",
    "    for item in tmp:\n",
    "        tmp_list.append(item[1])\n",
    "    training_data_tag_list.append(tmp_list)\n",
    "    \n",
    "print (len(training_data_tag_list))\n",
    "\n",
    "# Read the tags of the monpa\n",
    "with open ('./monpa_tag.txt' , 'r' , encoding = 'utf-8') as rf:\n",
    "    cnt = 0\n",
    "    for tag in rf.readlines():\n",
    "        dict_idx2tag[cnt] = tag.strip('\\n')\n",
    "        dict_tag2idx[tag.strip('\\n')] = cnt\n",
    "        cnt += 1  \n",
    "\n",
    "print ('The dictionary of idxs 2 tags : ', dict_idx2tag)\n",
    "print ('The dictionary of tags 2 idxs : ' , dict_tag2idx)\n",
    "\n",
    "shuffle_zip = list(zip(training_data_tag_list, training_target_sentence_list))\n",
    "random.shuffle(shuffle_zip)\n",
    "training_data_tag_list[:], training_target_sentence_list[:] = zip(*shuffle_zip)\n",
    "print (training_data_tag_list, training_target_sentence_list)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the training data list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"f = open ('../pickle/training_tags.pkl', 'wb')\\npickle.dump(training_data_tag_list, f)\\nf.close()\\nf = open ('../pickle/target.pkl', 'wb')\\npickle.dump(training_target_sentence_list, f)\\nf.close()\""
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''f = open ('../pickle/training_tags.pkl', 'wb')\n",
    "pickle.dump(training_data_tag_list, f)\n",
    "f.close()\n",
    "f = open ('../pickle/target.pkl', 'wb')\n",
    "pickle.dump(training_target_sentence_list, f)\n",
    "f.close()'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open ('../pickle/training_tags.pkl', 'rb')\n",
    "a = pickle.load(f)\n",
    "f.close()\n",
    "f = open ('../pickle/target.pkl', 'rb')\n",
    "b = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_idx2tag = dict()\n",
    "dict_tag2idx = dict()\n",
    "with open ('./monpa_tag.txt' , 'r' , encoding = 'utf-8') as rf:\n",
    "    cnt = 0\n",
    "    for tag in rf.readlines():\n",
    "        dict_idx2tag[cnt] = tag.strip('\\n')\n",
    "        dict_tag2idx[tag.strip('\\n')] = cnt\n",
    "        cnt += 1  \n",
    "training_data_tag_list = a\n",
    "training_target_sentence_list = b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparamters_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_v2 = 100\n",
    "learning_rate_v2 = 0.01\n",
    "input_size_v2 = len(dict_idx2tag.keys())\n",
    "gru_hidden_size_v2 = 8\n",
    "embedding_size_v2 = 16\n",
    "output_size_v2 = 2\n",
    "gru_layer_v2 = 1\n",
    "filter_num_1_v2 = 3\n",
    "filter_size_1_v2 = 3\n",
    "filter_num_2_v2 = 3\n",
    "filter_size_2_v2 = 2\n",
    "max_word_v2 = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定義訓練function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_v2():\n",
    "    model_v2 = Convolution_1D_Model(total_tag= input_size_v2, \n",
    "                                    embeding_size= embedding_size_v2, \n",
    "                                    filter_num_1= filter_num_1_v2, \n",
    "                                    filter_size_1= filter_size_1_v2, \n",
    "                                    filter_num_2= filter_num_2_v2, \n",
    "                                    filter_size_2=filter_size_2_v2 ,\n",
    "                                    output_size = output_size_v2, \n",
    "                                    max_word= max_word_v2 )\n",
    "    optimizer_v2 = torch.optim.SGD(model_v2.parameters(), lr=learning_rate_v2)\n",
    "    loss_function_v2 = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(epoch_v2):\n",
    "        sub_epoch = 0\n",
    "        total_loss = 0\n",
    "        for seq_tag_idx in range(len(training_data_tag_list)):\n",
    "            if len(training_data_tag_list[seq_tag_idx]) > max_word_v2:\n",
    "                continue\n",
    "            sub_epoch += 1\n",
    "            input_data = torch.LongTensor([dict_tag2idx[t] for t in training_data_tag_list[seq_tag_idx]])\n",
    "            target_data = torch.LongTensor([training_target_sentence_list[seq_tag_idx]])\n",
    "            out = model_v2(input_data)\n",
    "            out = out.unsqueeze(0)\n",
    "            loss = loss_function_v2(out, target_data)\n",
    "            total_loss += loss.item()\n",
    "            optimizer_v2.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer_v2.step()\n",
    "            '''\n",
    "            if sub_epoch % 100 == 0:\n",
    "                print ('Epoch '+  str(epoch) + ' '+ str(sub_epoch)+'/' + str(len(training_data_tag_list)) + '  Loss : ' + str(total_loss))\n",
    "            '''\n",
    "        print ('Epoch ' + str(epoch) + ' Loss : ' + str (total_loss) + ' total_train_data : ' + str(sub_epoch) )\n",
    "        torch.save(model_v2, '../pickle/model_v2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:41: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss : 4876.927813082933 total_train_data : 7014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type Convolution_1D_Model. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss : 4870.383149147034 total_train_data : 7014\n",
      "Epoch 2 Loss : 4870.043609023094 total_train_data : 7014\n",
      "Epoch 3 Loss : 4869.911008358002 total_train_data : 7014\n",
      "Epoch 4 Loss : 4869.86924213171 total_train_data : 7014\n",
      "Epoch 5 Loss : 4869.841605484486 total_train_data : 7014\n",
      "Epoch 6 Loss : 4869.816461861134 total_train_data : 7014\n",
      "Epoch 7 Loss : 4869.8019688129425 total_train_data : 7014\n",
      "Epoch 8 Loss : 4869.797327756882 total_train_data : 7014\n",
      "Epoch 9 Loss : 4869.795873999596 total_train_data : 7014\n",
      "Epoch 10 Loss : 4869.793142020702 total_train_data : 7014\n",
      "Epoch 11 Loss : 4869.787251055241 total_train_data : 7014\n",
      "Epoch 12 Loss : 4869.783266484737 total_train_data : 7014\n",
      "Epoch 13 Loss : 4869.786013841629 total_train_data : 7014\n",
      "Epoch 14 Loss : 4869.7828823924065 total_train_data : 7014\n",
      "Epoch 15 Loss : 4869.785557150841 total_train_data : 7014\n",
      "Epoch 16 Loss : 4869.781072199345 total_train_data : 7014\n",
      "Epoch 17 Loss : 4869.781697392464 total_train_data : 7014\n",
      "Epoch 18 Loss : 4869.783057153225 total_train_data : 7014\n",
      "Epoch 19 Loss : 4869.783206164837 total_train_data : 7014\n",
      "Epoch 20 Loss : 4869.778165757656 total_train_data : 7014\n",
      "Epoch 21 Loss : 4869.77826744318 total_train_data : 7014\n",
      "Epoch 22 Loss : 4869.783542513847 total_train_data : 7014\n",
      "Epoch 23 Loss : 4869.781677007675 total_train_data : 7014\n",
      "Epoch 24 Loss : 4869.779553472996 total_train_data : 7014\n",
      "Epoch 25 Loss : 4869.777904868126 total_train_data : 7014\n",
      "Epoch 26 Loss : 4869.777403712273 total_train_data : 7014\n",
      "Epoch 27 Loss : 4869.770434260368 total_train_data : 7014\n",
      "Epoch 28 Loss : 4869.76805305481 total_train_data : 7014\n",
      "Epoch 29 Loss : 4869.770639359951 total_train_data : 7014\n",
      "Epoch 30 Loss : 4869.770583808422 total_train_data : 7014\n",
      "Epoch 31 Loss : 4869.7696839571 total_train_data : 7014\n",
      "Epoch 32 Loss : 4869.768732249737 total_train_data : 7014\n",
      "Epoch 33 Loss : 4869.767870306969 total_train_data : 7014\n",
      "Epoch 34 Loss : 4869.767149627209 total_train_data : 7014\n",
      "Epoch 35 Loss : 4869.7659667134285 total_train_data : 7014\n",
      "Epoch 36 Loss : 4869.764290034771 total_train_data : 7014\n",
      "Epoch 37 Loss : 4869.7635261416435 total_train_data : 7014\n",
      "Epoch 38 Loss : 4869.7650391459465 total_train_data : 7014\n",
      "Epoch 39 Loss : 4869.764076769352 total_train_data : 7014\n",
      "Epoch 40 Loss : 4869.765888750553 total_train_data : 7014\n",
      "Epoch 41 Loss : 4869.76429516077 total_train_data : 7014\n",
      "Epoch 42 Loss : 4869.763980567455 total_train_data : 7014\n",
      "Epoch 43 Loss : 4869.763335287571 total_train_data : 7014\n",
      "Epoch 44 Loss : 4869.7636576890945 total_train_data : 7014\n",
      "Epoch 45 Loss : 4869.762636303902 total_train_data : 7014\n",
      "Epoch 46 Loss : 4869.760812401772 total_train_data : 7014\n",
      "Epoch 47 Loss : 4869.762399971485 total_train_data : 7014\n",
      "Epoch 48 Loss : 4869.7610711455345 total_train_data : 7014\n",
      "Epoch 49 Loss : 4869.76129424572 total_train_data : 7014\n",
      "Epoch 50 Loss : 4869.761240839958 total_train_data : 7014\n",
      "Epoch 51 Loss : 4869.759712398052 total_train_data : 7014\n",
      "Epoch 52 Loss : 4869.759892046452 total_train_data : 7014\n",
      "Epoch 53 Loss : 4869.759271323681 total_train_data : 7014\n",
      "Epoch 54 Loss : 4869.7606481313705 total_train_data : 7014\n",
      "Epoch 55 Loss : 4869.758912563324 total_train_data : 7014\n",
      "Epoch 56 Loss : 4869.7589982151985 total_train_data : 7014\n",
      "Epoch 57 Loss : 4869.758637368679 total_train_data : 7014\n",
      "Epoch 58 Loss : 4869.758831858635 total_train_data : 7014\n",
      "Epoch 59 Loss : 4869.7576813697815 total_train_data : 7014\n",
      "Epoch 60 Loss : 4869.759209036827 total_train_data : 7014\n",
      "Epoch 61 Loss : 4869.757175564766 total_train_data : 7014\n",
      "Epoch 62 Loss : 4869.757615864277 total_train_data : 7014\n",
      "Epoch 63 Loss : 4869.756859362125 total_train_data : 7014\n",
      "Epoch 64 Loss : 4869.756764173508 total_train_data : 7014\n",
      "Epoch 65 Loss : 4869.756353318691 total_train_data : 7014\n",
      "Epoch 66 Loss : 4869.7566893696785 total_train_data : 7014\n",
      "Epoch 67 Loss : 4869.756218731403 total_train_data : 7014\n",
      "Epoch 68 Loss : 4869.755222260952 total_train_data : 7014\n",
      "Epoch 69 Loss : 4869.7566588521 total_train_data : 7014\n",
      "Epoch 70 Loss : 4869.753857254982 total_train_data : 7014\n",
      "Epoch 71 Loss : 4869.753476679325 total_train_data : 7014\n",
      "Epoch 72 Loss : 4869.752221167088 total_train_data : 7014\n",
      "Epoch 73 Loss : 4869.755022585392 total_train_data : 7014\n",
      "Epoch 74 Loss : 4869.752908170223 total_train_data : 7014\n",
      "Epoch 75 Loss : 4869.752974808216 total_train_data : 7014\n",
      "Epoch 76 Loss : 4869.751688420773 total_train_data : 7014\n",
      "Epoch 77 Loss : 4869.752903282642 total_train_data : 7014\n",
      "Epoch 78 Loss : 4869.753442168236 total_train_data : 7014\n",
      "Epoch 79 Loss : 4869.75200676918 total_train_data : 7014\n",
      "Epoch 80 Loss : 4869.751438558102 total_train_data : 7014\n",
      "Epoch 81 Loss : 4869.75196492672 total_train_data : 7014\n",
      "Epoch 82 Loss : 4869.751535952091 total_train_data : 7014\n",
      "Epoch 83 Loss : 4869.75281047821 total_train_data : 7014\n",
      "Epoch 84 Loss : 4869.75183480978 total_train_data : 7014\n",
      "Epoch 85 Loss : 4869.751161873341 total_train_data : 7014\n",
      "Epoch 86 Loss : 4869.7511703372 total_train_data : 7014\n",
      "Epoch 87 Loss : 4869.752614498138 total_train_data : 7014\n",
      "Epoch 88 Loss : 4869.751046657562 total_train_data : 7014\n",
      "Epoch 89 Loss : 4869.750540316105 total_train_data : 7014\n",
      "Epoch 90 Loss : 4869.750922262669 total_train_data : 7014\n",
      "Epoch 91 Loss : 4869.751183271408 total_train_data : 7014\n",
      "Epoch 92 Loss : 4869.749597251415 total_train_data : 7014\n",
      "Epoch 93 Loss : 4869.749607205391 total_train_data : 7014\n",
      "Epoch 94 Loss : 4869.750627160072 total_train_data : 7014\n",
      "Epoch 95 Loss : 4869.750143647194 total_train_data : 7014\n",
      "Epoch 96 Loss : 4869.750256180763 total_train_data : 7014\n",
      "Epoch 97 Loss : 4869.749833405018 total_train_data : 7014\n",
      "Epoch 98 Loss : 4869.7489730119705 total_train_data : 7014\n",
      "Epoch 99 Loss : 4869.750437796116 total_train_data : 7014\n"
     ]
    }
   ],
   "source": [
    "train_v2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
