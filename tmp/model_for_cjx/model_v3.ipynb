{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the using package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import requests\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import monpa\n",
    "import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Architecture of the Designed Model using GRU and Convolution 1D\n",
    "\n",
    " <img src=\"https://i.imgur.com/JuOTx1S.png\" width = \"300\" height = \"200\" alt=\"design_model\" align=center />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design Model\n",
    "---\n",
    "#### Pytorch的Embedding:\n",
    "* 常設定參數：\n",
    "    * num_embeddings  : 字典的大小\n",
    "    * embedding_dim : 詞向量維度\n",
    "    \n",
    "* 輸入向量格式：(＊)  <br>\n",
    "* 輸出向量格式 : (＊ , H) <br>\n",
    "＊ : 長整數向量 , e.g. [40] 也就是該詞的index<br>\n",
    " H : 詞向量維度<br>\n",
    " \n",
    "---\n",
    "\n",
    "#### Pytorch的GRU:\n",
    "* 常設定參數：\n",
    "    * input_size  : 輸入向量的維度\n",
    "    * hidden_size : 隱藏層維度\n",
    "    * num_layers : 層數\n",
    "    * bidirectional : 是否雙向\n",
    "    \n",
    "* 輸入向量格式：(L , N , M)  <br>\n",
    "* 輸出向量格式 : (L , N , H) <br>\n",
    "\n",
    "L : 輸入的長度（Sequential length）<br>\n",
    "N : batch size\n",
    "M : input size\n",
    "H : hidden size\n",
    "\n",
    "---\n",
    "#### Pytorch的Conv1d:\n",
    "   * 常設定參數：\n",
    "        * in_channels : 輸入資料的維度\n",
    "        * out_channels : filter的數量\n",
    "        * kernal_size : filter的size\n",
    "   * 輸入向量格式：(N , Cin , L)\n",
    "   * 輸出向量格式 : (N , Cout , Lout)\n",
    "   \n",
    "N : batch size 。<br>\n",
    "Cin : 資料的維度數<br>\n",
    "Cout : filter數量<br>\n",
    "L : 輸入的長度（Sequential length）<br>\n",
    "Lout : 輸出的長度 (會根據filter size和stride的不同而有所不同)<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU_with_Conv1D_Model (nn.Module):\n",
    "    def __init__(self, total_tag, embeding_size, gru_hidden,gru_layer, filter_num_1, filter_size_1, filter_num_2, filter_size_2, output_size, max_word):\n",
    "        super(GRU_with_Conv1D_Model, self).__init__()\n",
    "        self.total_tag = total_tag\n",
    "        self.max_word = max_word\n",
    "        self.embeding_size = embeding_size\n",
    "        self.gru_hidden = gru_hidden\n",
    "        self.gru_layer = gru_layer\n",
    "        self.filter_num_1 = filter_num_1\n",
    "        self.filter_size_1 = filter_size_1\n",
    "        self.filter_num_2 = filter_num_2\n",
    "        self.filter_size_2 = filter_size_2\n",
    "        self.output_size = output_size\n",
    "        self.Embeding = nn.Embedding(self.total_tag, self.embeding_size)\n",
    "        self.GRU = nn.GRU(self.embeding_size , self.gru_hidden, self.gru_layer, bidirectional = True)\n",
    "        self.Conv1d_layer1 = nn.Sequential(nn.Conv1d(in_channels=self.gru_hidden*2, \n",
    "                                                            out_channels=self.filter_num_1,\n",
    "                                                            kernel_size=self.filter_size_1),\n",
    "                                                         #nn.BatchNorm1d (self.filter_size_1),\n",
    "                                                         nn.ReLU())\n",
    "        \n",
    "        self.Conv1d_layer2 = nn.Sequential(nn.Conv1d(in_channels=self.filter_num_1, \n",
    "                                                            out_channels=self.filter_num_2,\n",
    "                                                            kernel_size=self.filter_size_2),\n",
    "                                                         #nn.BatchNorm1d (self.filter_size_2),\n",
    "                                                         nn.ReLU())\n",
    "        # batch size is  set to 1 , output_size is set to 2\n",
    "        L = (self.max_word- self.filter_size_1+ 1)- self.filter_size_2+ 1\n",
    "        self.linear = nn.Linear( self.filter_num_2 * L, self.output_size)\n",
    "        \n",
    "    def forward(self, input_data):  \n",
    "        embed_res = self.Embeding (input_data)\n",
    "        if len(input_data) < self.max_word:\n",
    "            for c in range (self.max_word - len(input_data)):\n",
    "                embed_res = torch.cat((embed_res, torch.zeros(1, self.embeding_size)),dim = 0 )\n",
    "        #print (' Embedding :::: ' , embed_res.size())\n",
    "        # let the input dimension be the (L , N , M) and the get the output with the dimension (L , N , H)\n",
    "        gru_res, _ = self.GRU(embed_res.unsqueeze(1))\n",
    "        #print (' GRU :::: ' , gru_res.size())\n",
    "        # let the input dimension be the (N , Cin , L) and then the output with the dimension (N, Cout , Lout)\n",
    "        conv_res1 = self.Conv1d_layer1 (gru_res.permute(1,2,0))\n",
    "        #print (' conv_res1 :::: ' , conv_res1.size())\n",
    "        conv_res2 = self.Conv1d_layer2 (conv_res1)\n",
    "        #print ('conv_res2 :::: ' , conv_res2.size())\n",
    "        linear_res = self.linear(conv_res2.view(-1))\n",
    "        final_res = F.softmax(linear_res)\n",
    "        return final_res\n",
    "            \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the code\n",
    "model_v3 = GRU_with_Conv1D_Model(total_tag= 16, embeding_size=8, gru_hidden= 6 , gru_layer=1, filter_num_1=5, filter_size_1=2, filter_num_2 = 5, filter_size_2 = 2,output_size = 7, max_word = 10)\n",
    "test_vec = torch.LongTensor([1,2,3,4,5,6,7])\n",
    "model_v3 (test_vec)\n",
    "print ('V3_Model_with_conv_and_gru ::: \\n', model_v3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing\n",
    "\n",
    "* 將positive_data和negative_data載入存入List\n",
    "* 將monpa的tags載入dictionary\n",
    "* 將sentence 的 words 轉成詞性\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''training_data_sentence_list = list()\n",
    "training_data_tag_list = list()\n",
    "training_target_sentence_list = list()\n",
    "dict_idx2tag = dict()\n",
    "dict_tag2idx = dict()\n",
    "\n",
    "# Read both the postive data and negative data\n",
    "with open ('../CJX_Train_Test_Data/positive_data.txt', 'r', encoding = 'utf-8') as rf:\n",
    "    for pos_s in rf.readlines():\n",
    "        training_data_sentence_list.append(pos_s.strip('\\n').strip(',').strip('，').strip('《').strip('》').strip('【').strip('】').strip('、').strip('、').strip('-'))\n",
    "        training_target_sentence_list.append(1)\n",
    "with open ('../CJX_Train_Test_Data/positive_data.txt', 'r', encoding = 'utf-8') as rf:\n",
    "    for pos_s in rf.readlines():\n",
    "        training_data_sentence_list.append(pos_s.strip('\\n').strip(',').strip('，').strip('《').strip('》').strip('【').strip('】').strip('、').strip('-'))\n",
    "        training_target_sentence_list.append(0)\n",
    "        \n",
    "print ('training data size : ',len(training_data_sentence_list))\n",
    "print ('training data target size : ',len(training_target_sentence_list))\n",
    "tmp_cnt = 0\n",
    "# Change the words to tags, because we need to use the sequential tags for training.\n",
    "for sentence in training_data_sentence_list:\n",
    "    tmp_cnt += 1\n",
    "    #print (tmp_cnt)\n",
    "    tmp = monpa.pseg(sentence)\n",
    "    tmp_list = list()\n",
    "    for item in tmp:\n",
    "        tmp_list.append(item[1])\n",
    "    training_data_tag_list.append(tmp_list)\n",
    "    \n",
    "print (len(training_data_tag_list))\n",
    "\n",
    "# Read the tags of the monpa\n",
    "with open ('./monpa_tag.txt' , 'r' , encoding = 'utf-8') as rf:\n",
    "    cnt = 0\n",
    "    for tag in rf.readlines():\n",
    "        dict_idx2tag[cnt] = tag.strip('\\n')\n",
    "        dict_tag2idx[tag.strip('\\n')] = cnt\n",
    "        cnt += 1  \n",
    "\n",
    "print ('The dictionary of idxs 2 tags : ', dict_idx2tag)\n",
    "print ('The dictionary of tags 2 idxs : ' , dict_tag2idx)\n",
    "\n",
    "shuffle_zip = list(zip(training_data_tag_list, training_target_sentence_list))\n",
    "random.shuffle(shuffle_zip)\n",
    "training_data_tag_list[:], training_target_sentence_list[:] = zip(*shuffle_zip)\n",
    "print (training_data_tag_list, training_target_sentence_list)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the training data list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''f = open ('../pickle/training_tags.pkl', 'wb')\n",
    "pickle.dump(training_data_tag_list, f)\n",
    "f.close()\n",
    "f = open ('../pickle/target.pkl', 'wb')\n",
    "pickle.dump(training_target_sentence_list, f)\n",
    "f.close()'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open ('../pickle/training_tags.pkl', 'rb')\n",
    "a = pickle.load( f)\n",
    "f.close()\n",
    "f = open ('../pickle/target.pkl', 'rb')\n",
    "b = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_idx2tag = dict()\n",
    "dict_tag2idx = dict()\n",
    "with open ('./monpa_tag.txt' , 'r' , encoding = 'utf-8') as rf:\n",
    "    cnt = 0\n",
    "    for tag in rf.readlines():\n",
    "        dict_idx2tag[cnt] = tag.strip('\\n')\n",
    "        dict_tag2idx[tag.strip('\\n')] = cnt\n",
    "        cnt += 1  \n",
    "training_data_tag_list = a\n",
    "training_target_sentence_list = b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparamters_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_v3 = 10\n",
    "learning_rate_v3 = 0.000001\n",
    "input_size_v3 = len(dict_idx2tag.keys())\n",
    "gru_hidden_size_v3 = 8\n",
    "embedding_size_v3 = 32\n",
    "output_size_v3 = 2\n",
    "gru_layer_v3 = 1\n",
    "filter_num_1_v3 = 3\n",
    "filter_size_1_v3 = 3\n",
    "filter_num_2_v3 = 3\n",
    "filter_size_2_v3 = 3\n",
    "max_word_v3 = 20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定義訓練function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_v3():\n",
    "    model_v3 = GRU_with_Conv1D_Model(total_tag= input_size_v3, \n",
    "                                    embeding_size= embedding_size_v3, \n",
    "                                    filter_num_1= filter_num_1_v3, \n",
    "                                    filter_size_1= filter_size_1_v3, \n",
    "                                    filter_num_2= filter_num_2_v3, \n",
    "                                    filter_size_2=filter_size_2_v3 ,\n",
    "                                    output_size = output_size_v3, \n",
    "                                    max_word= max_word_v3,\n",
    "                                    gru_hidden = gru_hidden_size_v3,\n",
    "                                    gru_layer = gru_layer_v3)\n",
    "    optimizer_v3 = torch.optim.SGD(model_v3.parameters(), lr=learning_rate_v3)\n",
    "    loss_function_v3 = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(epoch_v3):\n",
    "        sub_epoch = 0\n",
    "        total_loss = 0\n",
    "        for seq_tag_idx in range(len(training_data_tag_list)):\n",
    "            if len(training_data_tag_list[seq_tag_idx]) > max_word_v3:\n",
    "                continue\n",
    "            sub_epoch += 1\n",
    "            input_data = torch.LongTensor([dict_tag2idx[t] for t in training_data_tag_list[seq_tag_idx]])\n",
    "            target_data = torch.LongTensor([training_target_sentence_list[seq_tag_idx]])\n",
    "            out = model_v3(input_data)\n",
    "            out = out.unsqueeze(0)\n",
    "            loss = loss_function_v3(out, target_data)\n",
    "            total_loss += loss.item()\n",
    "            optimizer_v3.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer_v3.step()\n",
    "            '''\n",
    "            if sub_epoch % 100 == 0:\n",
    "                print ('Epoch '+  str(epoch) + ' '+ str(sub_epoch)+'/' + str(len(training_data_tag_list)) + '  Loss : ' + str(loss))\n",
    "            '''\n",
    "            \n",
    "        print ('Epoch ' + str(epoch) + ' Loss : ' + str (total_loss))    \n",
    "        torch.save(model_v3, '../pickle/model_v3.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:46: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss : 4866.513285040855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type GRU_with_Conv1D_Model. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss : 4866.500924646854\n",
      "Epoch 2 Loss : 4866.488595545292\n",
      "Epoch 3 Loss : 4866.476292550564\n",
      "Epoch 4 Loss : 4866.464019477367\n",
      "Epoch 5 Loss : 4866.451784431934\n",
      "Epoch 6 Loss : 4866.439587235451\n",
      "Epoch 7 Loss : 4866.427417695522\n",
      "Epoch 8 Loss : 4866.4152772426605\n",
      "Epoch 9 Loss : 4866.403172314167\n"
     ]
    }
   ],
   "source": [
    "train_v3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
